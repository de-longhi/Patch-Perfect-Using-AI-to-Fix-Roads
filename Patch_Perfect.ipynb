{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-ebO8XbEsEd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion() # (%matplotlib inline)\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy0o8aeIEsEf"
      },
      "source": [
        "# Patch Perfect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC__VfxoEsEi"
      },
      "source": [
        "This notebook will document the process we underwent to find a solution to the plothole-problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZOHtzy0EsEj"
      },
      "source": [
        "\n",
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY7iO0TbEsEj"
      },
      "source": [
        "We start by looking at the data systematically to see where we will inevitably need to solve problems before we create a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hVhsQ812EsEk"
      },
      "outputs": [],
      "source": [
        "# __file__ = os.path.abspath('') # notebooks are stupid\n",
        "# DATA_DIR = Path(__file__).resolve() / \"data\"\n",
        "# TRAIN_LABELS_PATH = DATA_DIR / \"train_labels.csv\"\n",
        "\n",
        "# train_label_df = pd.read_csv(filepath_or_buffer=TRAIN_LABELS_PATH)\n",
        "# train_label_df.rename(columns={'Bags used ': 'Bags used'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HfVHP5EDEsEl",
        "outputId": "81635175-1a4b-4146-cdea-b6fac2347de5"
      },
      "outputs": [],
      "source": [
        "# values = train_label_df.loc[:, 'Bags used'].value_counts()\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.hist(values, bins=range(1, max(values) + 2), edgecolor='black')\n",
        "# plt.title('Histogram of Data Points per Bag Amount')\n",
        "# plt.xlabel('Number of Data Points for Each Bag Amount')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMcpWCujEsEn"
      },
      "source": [
        "We can see that there is a massive class imbalance. This could create issues where a model trained on this dataset has a bias towards more common bags. Most values are between 0 and 1 with some values much higher. There are many strategies we could use to solve this, including but not limited to:\n",
        "<ul>\n",
        "<li>Some label abstraction technique where we might create labels based on a logarithmic scale</li>\n",
        "<li>Data augmentation as a class imbalance mitigation: This process is called upsampling</li>\n",
        "</ul>\n",
        "\n",
        "We should also consider the following: The data makes this problem seem like a regression model is needed, but tuning the labels may enable us to change it to a much simpler classification task at the cost of some accuracy. Doing this would result in a much more robust model and enable us to use techniques like label smoothing to let the model generalize more to unseen data.\n",
        "<hr>\n",
        "References:\n",
        "<ul>\n",
        "<li>Paperspace Blog. (2022). Data Augmentation: A Class Imbalance Mitigative Measure. [online] Available at: https://blog.paperspace.com/data-augmentation-a-class-imbalance-mitigative-measure/.</li>\n",
        "<li>S. Wang and X. Yao, \"Multiclass Imbalance Problems: Analysis and Potential Solutions,\" in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 42, no. 4, pp. 1119-1130, Aug. 2012, doi: 10.1109/TSMCB.2012.2187280. keywords: {Training;Correlation;Training data;Pattern analysis;Genetic algorithms;IEEE Potentials;Cybernetics;Boosting;diversity;ensemble learning;multiclass imbalance problems;negative correlation learning}, </li>\n",
        "</ul>\n",
        "â€Œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsDHOvH0EsEp"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kqPHOll3EsEq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_977964/3933020813.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(\"./pretrained_model.pt\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "model = torch.load(\"./pretrained_model.pt\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "      param.requires_grad = False\n",
        "\n",
        "\n",
        "model.classifier = nn.Sequential( # Change only the classifier of the model, I.E the last few layers\n",
        "\n",
        "    nn.Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "    nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1, inplace=False),\n",
        "    nn.Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1)) # Change the output to 3 classes instead of 21\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s-gk4B3EsEr",
        "outputId": "5ea90a64-eca6-41fd-bc95-9272a1567577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable layers: 75\n",
            "Non-trainable layers: 87\n"
          ]
        }
      ],
      "source": [
        "trainable_layers = 0\n",
        "non_trainable_layers = 0\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    # Check if any parameter in the layer requires gradients\n",
        "    if any(param.requires_grad for param in module.parameters()):\n",
        "        trainable_layers += 1\n",
        "    else:\n",
        "        non_trainable_layers += 1\n",
        "\n",
        "print(f\"Trainable layers: {trainable_layers}\")\n",
        "print(f\"Non-trainable layers: {non_trainable_layers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoRmM5y3EsEr",
        "outputId": "322824e9-04f0-4f38-cd24-5baf884d4369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 9485187\n",
            "Non-trainable parameters: 25827797\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trainable_params = 0\n",
        "non_trainable_params = 0\n",
        "\n",
        "for param in model.parameters():\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()  # Count the number of elements\n",
        "    else:\n",
        "        non_trainable_params += param.numel()\n",
        "\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Non-trainable parameters: {non_trainable_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSg-1m7EsEt"
      },
      "source": [
        "## Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ns4tJ9HoEsEt"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "# optimizer = optim.Adam(\n",
        "#     filter(lambda p: p.requires_grad, model.parameters()),\n",
        "#     lr=0.001\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=1):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = 3e-4) # Karpathy's number\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            # Move data to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.type(torch.LongTensor).cuda()\n",
        "                model.cuda()\n",
        "\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)['out']\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #running_loss += loss.item() * images.size(0)\n",
        "        test_loss = evaluate_model(model, val_loader)\n",
        "        #epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Accuracy: {test_loss:.4f}')\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            total_pixels += labels.numel()\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "\n",
        "        accuracy = total_correct / total_pixels\n",
        "        return accuracy\n",
        "        #print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmklZldlEsEu"
      },
      "source": [
        "## Data Prep Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ClR6aVHHEsEu"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class for your data\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "rgb_to_class = {\n",
        "    (0, 0, 0): 0,\n",
        "    (255, 255, 255): 1,\n",
        "    (100, 100, 100): 2\n",
        "}\n",
        "\n",
        "class JointTransform:\n",
        "    def __init__(self, image_transforms=None, mask_transforms=None):\n",
        "        self.image_transforms = image_transforms\n",
        "        self.mask_transforms = mask_transforms\n",
        "    @staticmethod\n",
        "    def set_seed(seed):\n",
        "      torch.manual_seed(seed)\n",
        "      torch.cuda.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "      random.seed(seed)\n",
        "      np.random.seed(seed)\n",
        "\n",
        "      # Ensure that all operations are deterministic\n",
        "      torch.backends.cudnn.deterministic = True\n",
        "      torch.backends.cudnn.benchmark = False\n",
        "    def __call__(self, image, mask):\n",
        "        if self.image_transforms:\n",
        "            seed = random.randint(0, 2**32)\n",
        "            self.set_seed(seed)\n",
        "            image = self.image_transforms(image)\n",
        "\n",
        "            self.set_seed(seed)\n",
        "            mask = self.mask_transforms(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "image_augmentations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "mask_augmentations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "# Instantiate joint transform\n",
        "augmentation = JointTransform(image_transforms=image_augmentations, mask_transforms=mask_augmentations)\n",
        "\n",
        "\n",
        "class Potholes(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, do_augmentation = True):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_filenames = os.listdir(image_dir)\n",
        "        self.mean = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "        self.do_augmentation = do_augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    @staticmethod\n",
        "    def rgb_to_mask(mask):\n",
        "        # Convert the mask to a numpy array\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # Initialize an array to hold the class indices\n",
        "        class_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)\n",
        "\n",
        "        # Apply the mapping from RGB values to class indices\n",
        "        for rgb, class_index in rgb_to_class.items():\n",
        "            matches = np.all(mask == rgb, axis=-1)\n",
        "            class_mask[matches] = class_index\n",
        "\n",
        "        return class_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, (img_name[:-4] + \"_mask.png\"))\n",
        "\n",
        "        # Load image and label\n",
        "        image = Image.open(img_path)\n",
        "        mask = Image.open(label_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply augmentations then transformations\n",
        "\n",
        "        if self.do_augmentation: image, mask = augmentation(image, mask)\n",
        "\n",
        "        image, mask = self.image_transforms(image, mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Define transforms for data augmentation and normalization\n",
        "\n",
        "    def image_transforms(self, image, label):\n",
        "        transform_images = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Resize to the desired input size\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "        transform_labels = transforms.Compose([\n",
        "            transforms.Resize((256, 256))\n",
        "        ])\n",
        "        mask = self.rgb_to_mask(transform_labels(label))\n",
        "        return transform_images(image), mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Paths to your dataset\n",
        "train_image_dir = \"./data/train_images_segmented/\"\n",
        "train_label_dir = \"./data/train_masks_segmented/\"\n",
        "\n",
        "val_image_dir = \"./data/validation set/\"\n",
        "val_label_dir = \"./data/validation masks/\"\n",
        "\n",
        "# size of stick/image indicates camera zoom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "gkGh36k8EsEu",
        "outputId": "a51b5930-8b0e-446c-fb47-b51823ad840f"
      },
      "outputs": [],
      "source": [
        "# Create datasets and data loaders\n",
        "train_dataset = Potholes(train_image_dir, train_label_dir, do_augmentation=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "val_dataset = Potholes(val_image_dir, val_label_dir, do_augmentation=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "#train_model(model = model,train_loader = train_loader,num_epochs= 1000, val_loader=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ExDTpR72fYNV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0):\n",
        "    model.to(\"cuda\")\n",
        "    number_in_epoch = len(train_loader) - 1\n",
        "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
        "    lr = init_value\n",
        "    optimizer.param_groups[0][\"lr\"] = lr\n",
        "    best_loss = float('inf')\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        batch_num += 1\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs, labels\n",
        "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)[\"out\"]\n",
        "        loss = loss_fn(outputs, labels.type(torch.LongTensor).cuda())\n",
        "\n",
        "        # Convert loss to float for comparison and storage\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        # Crash out if loss explodes\n",
        "        if batch_num > 1 and loss_value > 4 * best_loss:\n",
        "            return log_lrs[10:-5], losses[10:-5]\n",
        "\n",
        "        # Record the best loss\n",
        "        if loss_value < best_loss:\n",
        "            best_loss = loss_value\n",
        "\n",
        "        # Store the values\n",
        "        losses.append(loss_value)\n",
        "        log_lrs.append(math.log10(lr))\n",
        "\n",
        "        # Do the backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the lr for the next step and store\n",
        "        lr *= update_step\n",
        "        optimizer.param_groups[0][\"lr\"] = lr\n",
        "\n",
        "    return log_lrs[10:-5], losses[10:-5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#logs,losses = find_lr(model, nn.CrossEntropyLoss(), optim.Adam(model.parameters(), lr = 3e-4), train_loader=train_loader)\n",
        "#plt.plot(logs,losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_977964/1325712498.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  final_model = torch.load(\"./trained_model.pt\")\n",
            "/home/johan/miniconda3/envs/hackathon/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./trained_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/torch/serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[1;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/torch/serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/torch/serialization.py:1466\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1461\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1466\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1467\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1468\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1471\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/torch/serialization.py:414\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[0;32m--> 391\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/torch/serialization.py:364\u001b[0m, in \u001b[0;36m_validate_device\u001b[0;34m(location, backend_name)\u001b[0m\n\u001b[1;32m    362\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    365\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    366\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    367\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    368\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    370\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "final_model = torch.load(\"./trained_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
