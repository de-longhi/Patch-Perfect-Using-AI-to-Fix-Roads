{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i-ebO8XbEsEd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion() # (%matplotlib inline)\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy0o8aeIEsEf"
      },
      "source": [
        "# Patch Perfect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC__VfxoEsEi"
      },
      "source": [
        "This notebook will document the process we underwent to find a solution to the plothole-problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZOHtzy0EsEj"
      },
      "source": [
        "\n",
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY7iO0TbEsEj"
      },
      "source": [
        "We start by looking at the data systematically to see where we will inevitably need to solve problems before we create a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hVhsQ812EsEk"
      },
      "outputs": [],
      "source": [
        "# __file__ = os.path.abspath('') # notebooks are stupid\n",
        "# DATA_DIR = Path(__file__).resolve() / \"data\"\n",
        "# TRAIN_LABELS_PATH = DATA_DIR / \"train_labels.csv\"\n",
        "\n",
        "# train_label_df = pd.read_csv(filepath_or_buffer=TRAIN_LABELS_PATH)\n",
        "# train_label_df.rename(columns={'Bags used ': 'Bags used'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HfVHP5EDEsEl",
        "outputId": "81635175-1a4b-4146-cdea-b6fac2347de5"
      },
      "outputs": [],
      "source": [
        "# values = train_label_df.loc[:, 'Bags used'].value_counts()\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.hist(values, bins=range(1, max(values) + 2), edgecolor='black')\n",
        "# plt.title('Histogram of Data Points per Bag Amount')\n",
        "# plt.xlabel('Number of Data Points for Each Bag Amount')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMcpWCujEsEn"
      },
      "source": [
        "We can see that there is a massive class imbalance. This could create issues where a model trained on this dataset has a bias towards more common bags. Most values are between 0 and 1 with some values much higher. There are many strategies we could use to solve this, including but not limited to:\n",
        "<ul>\n",
        "<li>Some label abstraction technique where we might create labels based on a logarithmic scale</li>\n",
        "<li>Data augmentation as a class imbalance mitigation: This process is called upsampling</li>\n",
        "</ul>\n",
        "\n",
        "We should also consider the following: The data makes this problem seem like a regression model is needed, but tuning the labels may enable us to change it to a much simpler classification task at the cost of some accuracy. Doing this would result in a much more robust model and enable us to use techniques like label smoothing to let the model generalize more to unseen data.\n",
        "<hr>\n",
        "References:\n",
        "<ul>\n",
        "<li>Paperspace Blog. (2022). Data Augmentation: A Class Imbalance Mitigative Measure. [online] Available at: https://blog.paperspace.com/data-augmentation-a-class-imbalance-mitigative-measure/.</li>\n",
        "<li>S. Wang and X. Yao, \"Multiclass Imbalance Problems: Analysis and Potential Solutions,\" in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 42, no. 4, pp. 1119-1130, Aug. 2012, doi: 10.1109/TSMCB.2012.2187280. keywords: {Training;Correlation;Training data;Pattern analysis;Genetic algorithms;IEEE Potentials;Cybernetics;Boosting;diversity;ensemble learning;multiclass imbalance problems;negative correlation learning}, </li>\n",
        "</ul>\n",
        "â€Œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsDHOvH0EsEp"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kqPHOll3EsEq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "model = torch.load(\"./pretrained_model.pt\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "      param.requires_grad = False\n",
        "\n",
        "\n",
        "model.classifier = nn.Sequential( # Change only the classifier of the model, I.E the last few layers\n",
        "\n",
        "    nn.Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "    nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1, inplace=False),\n",
        "    nn.Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1)) # Change the output to 3 classes instead of 21\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s-gk4B3EsEr",
        "outputId": "5ea90a64-eca6-41fd-bc95-9272a1567577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable layers: 75\n",
            "Non-trainable layers: 87\n"
          ]
        }
      ],
      "source": [
        "trainable_layers = 0\n",
        "non_trainable_layers = 0\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    # Check if any parameter in the layer requires gradients\n",
        "    if any(param.requires_grad for param in module.parameters()):\n",
        "        trainable_layers += 1\n",
        "    else:\n",
        "        non_trainable_layers += 1\n",
        "\n",
        "print(f\"Trainable layers: {trainable_layers}\")\n",
        "print(f\"Non-trainable layers: {non_trainable_layers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoRmM5y3EsEr",
        "outputId": "322824e9-04f0-4f38-cd24-5baf884d4369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 9485187\n",
            "Non-trainable parameters: 25827797\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trainable_params = 0\n",
        "non_trainable_params = 0\n",
        "\n",
        "for param in model.parameters():\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()  # Count the number of elements\n",
        "    else:\n",
        "        non_trainable_params += param.numel()\n",
        "\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Non-trainable parameters: {non_trainable_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSg-1m7EsEt"
      },
      "source": [
        "## Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ns4tJ9HoEsEt"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "# optimizer = optim.Adam(\n",
        "#     filter(lambda p: p.requires_grad, model.parameters()),\n",
        "#     lr=0.001\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=1):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = 3e-4) # Karpathy's number\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            # Move data to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.type(torch.LongTensor).cuda()\n",
        "                model.cuda()\n",
        "\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)['out']\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #running_loss += loss.item() * images.size(0)\n",
        "        test_loss = evaluate_model(model, val_loader)\n",
        "        #epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Accuracy: {test_loss:.4f}')\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            total_pixels += labels.numel()\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "\n",
        "        accuracy = total_correct / total_pixels\n",
        "        return accuracy\n",
        "        #print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmklZldlEsEu"
      },
      "source": [
        "## Data Prep Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ClR6aVHHEsEu"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class for your data\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "rgb_to_class = {\n",
        "    (0, 0, 0): 0,\n",
        "    (255, 255, 255): 1,\n",
        "    (100, 100, 100): 2\n",
        "}\n",
        "\n",
        "class JointTransform:\n",
        "    def __init__(self, image_transforms=None, mask_transforms=None):\n",
        "        self.image_transforms = image_transforms\n",
        "        self.mask_transforms = mask_transforms\n",
        "    @staticmethod\n",
        "    def set_seed(seed):\n",
        "      torch.manual_seed(seed)\n",
        "      torch.cuda.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "      random.seed(seed)\n",
        "      np.random.seed(seed)\n",
        "\n",
        "      # Ensure that all operations are deterministic\n",
        "      torch.backends.cudnn.deterministic = True\n",
        "      torch.backends.cudnn.benchmark = False\n",
        "    def __call__(self, image, mask):\n",
        "        if self.image_transforms:\n",
        "            seed = random.randint(0, 2**32)\n",
        "            self.set_seed(seed)\n",
        "            image = self.image_transforms(image)\n",
        "\n",
        "            self.set_seed(seed)\n",
        "            mask = self.mask_transforms(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "image_augmentations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "mask_augmentations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "# Instantiate joint transform\n",
        "augmentation = JointTransform(image_transforms=image_augmentations, mask_transforms=mask_augmentations)\n",
        "\n",
        "\n",
        "class Potholes(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, do_augmentation = True):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_filenames = os.listdir(image_dir)\n",
        "        self.mean = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "        self.do_augmentation = do_augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    @staticmethod\n",
        "    def rgb_to_mask(mask):\n",
        "        # Convert the mask to a numpy array\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # Initialize an array to hold the class indices\n",
        "        class_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)\n",
        "\n",
        "        # Apply the mapping from RGB values to class indices\n",
        "        for rgb, class_index in rgb_to_class.items():\n",
        "            matches = np.all(mask == rgb, axis=-1)\n",
        "            class_mask[matches] = class_index\n",
        "\n",
        "        return class_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, (img_name[:-4] + \"_mask.png\"))\n",
        "\n",
        "        # Load image and label\n",
        "        image = Image.open(img_path)\n",
        "        mask = Image.open(label_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply augmentations then transformations\n",
        "\n",
        "        if self.do_augmentation: image, mask = augmentation(image, mask)\n",
        "\n",
        "        image, mask = self.image_transforms(image, mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Define transforms for data augmentation and normalization\n",
        "\n",
        "    def image_transforms(self, image, label):\n",
        "        transform_images = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Resize to the desired input size\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "        transform_labels = transforms.Compose([\n",
        "            transforms.Resize((256, 256))\n",
        "        ])\n",
        "        mask = self.rgb_to_mask(transform_labels(label))\n",
        "        return transform_images(image), mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Paths to your dataset\n",
        "train_image_dir = \"./data/train_images_segmented/\"\n",
        "train_label_dir = \"./data/train_masks_segmented/\"\n",
        "\n",
        "val_image_dir = \"./data/validation set/\"\n",
        "val_label_dir = \"./data/validation masks/\"\n",
        "\n",
        "# size of stick/image indicates camera zoom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "gkGh36k8EsEu",
        "outputId": "a51b5930-8b0e-446c-fb47-b51823ad840f"
      },
      "outputs": [],
      "source": [
        "# Create datasets and data loaders\n",
        "train_dataset = Potholes(train_image_dir, train_label_dir, do_augmentation=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "val_dataset = Potholes(val_image_dir, val_label_dir, do_augmentation=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "#train_model(model = model,train_loader = train_loader,num_epochs= 1000, val_loader=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ExDTpR72fYNV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0):\n",
        "    model.to(\"cuda\")\n",
        "    number_in_epoch = len(train_loader) - 1\n",
        "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
        "    lr = init_value\n",
        "    optimizer.param_groups[0][\"lr\"] = lr\n",
        "    best_loss = float('inf')\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        batch_num += 1\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs, labels\n",
        "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)[\"out\"]\n",
        "        loss = loss_fn(outputs, labels.type(torch.LongTensor).cuda())\n",
        "\n",
        "        # Convert loss to float for comparison and storage\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        # Crash out if loss explodes\n",
        "        if batch_num > 1 and loss_value > 4 * best_loss:\n",
        "            return log_lrs[10:-5], losses[10:-5]\n",
        "\n",
        "        # Record the best loss\n",
        "        if loss_value < best_loss:\n",
        "            best_loss = loss_value\n",
        "\n",
        "        # Store the values\n",
        "        losses.append(loss_value)\n",
        "        log_lrs.append(math.log10(lr))\n",
        "\n",
        "        # Do the backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the lr for the next step and store\n",
        "        lr *= update_step\n",
        "        optimizer.param_groups[0][\"lr\"] = lr\n",
        "\n",
        "    return log_lrs[10:-5], losses[10:-5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#logs,losses = find_lr(model, nn.CrossEntropyLoss(), optim.Adam(model.parameters(), lr = 3e-4), train_loader=train_loader)\n",
        "#plt.plot(logs,losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model = torch.load(\"./trained_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "test_transform = transforms.Compose([\n",
        "            transforms.Resize((448, 448)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "train_transform = transforms.Compose([\n",
        "            transforms.Resize((448, 448)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "test_image = Image.open(\"./data/test set resized/p103.jpg\").convert(\"RGB\")\n",
        "test_image = test_transform(test_image).cuda().view(1, 3, 448, 448)\n",
        "final_model.eval()\n",
        "out = final_model(test_image)[\"out\"]\n",
        "predicted_mask = torch.argmax(out, dim=1).view(448, 448).cpu()\n",
        "\n",
        "\n",
        "def array_to_image(array):\n",
        "    # Define the mapping from array values to RGB colors\n",
        "    color_mapping = {\n",
        "        0: (0, 0, 0),       # Black\n",
        "        1: (255, 255, 255), # White\n",
        "        2: (128, 128, 128)  # Gray\n",
        "    }\n",
        "    \n",
        "    # Convert the array to an RGB image\n",
        "    height, width = array.shape\n",
        "    image = Image.new('RGB', (width, height))\n",
        "    \n",
        "    for y in range(height):\n",
        "        for x in range(width):\n",
        "            image.putpixel((x, y), color_mapping[array[y, x]])\n",
        "    \n",
        "    return image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class pothole_estimator():\n",
        "    def __init__(self, array):\n",
        "        self.array = array\n",
        "        \n",
        "    def is_valid_two(self, y, x):\n",
        "        \"\"\"Check if the element at (y, x) is a 2 with both 0 and 1 as neighbors.\"\"\"\n",
        "        height, width = self.array.shape\n",
        "        neighbors = []\n",
        "        \n",
        "        # Check all 8 possible neighbors\n",
        "        for dy in [-1, 0, 1]:\n",
        "            for dx in [-1, 0, 1]:\n",
        "                if dy == 0 and dx == 0:\n",
        "                    continue\n",
        "                ny, nx = y + dy, x + dx\n",
        "                if 0 <= ny < height and 0 <= nx < width:\n",
        "                    neighbors.append(self.array[ny, nx])\n",
        "        \n",
        "        return 0 in neighbors and 1 in neighbors\n",
        "    \n",
        "\n",
        "    def find_closest_pair(self, coords):\n",
        "        \"\"\"Find the pair of points in coords that are closest to each other.\"\"\"\n",
        "        min_dist = float('inf')\n",
        "        closest_pair = None\n",
        "        \n",
        "        for i in range(len(coords)):\n",
        "            for j in range(i + 1, len(coords)):\n",
        "                if self.is_adjacent(coords[i], coords[j]):\n",
        "                    continue\n",
        "                dist = np.linalg.norm(np.array(coords[i]) - np.array(coords[j]))\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    closest_pair = (coords[i], coords[j])\n",
        "    \n",
        "        return closest_pair\n",
        "        \"\"\"Draw a line of 0's between points p1 and p2 using Bresenham's line algorithm.\"\"\"\n",
        "    def draw_line(self, p1, p2):\n",
        "        y1, x1 = p1\n",
        "        y2, x2 = p2\n",
        "        dy = abs(y2 - y1)\n",
        "        dx = abs(x2 - x1)\n",
        "        sy = 1 if y1 < y2 else -1\n",
        "        sx = 1 if x1 < x2 else -1\n",
        "        err = dx - dy\n",
        "\n",
        "        while True:\n",
        "            self.array[y1, x1] = 1\n",
        "            if (y1, x1) == (y2, x2):\n",
        "                break\n",
        "            e2 = 2 * err\n",
        "            if e2 > -dy:\n",
        "                err -= dy\n",
        "                x1 += sx\n",
        "            if e2 < dx:\n",
        "                err += dx\n",
        "                y1 += sy\n",
        "    @staticmethod        \n",
        "    def is_adjacent(p1, p2):\n",
        "        \"\"\"Check if two points p1 and p2 are adjacent (including diagonally).\"\"\"\n",
        "        y1, x1 = p1\n",
        "        y2, x2 = p2\n",
        "        return abs(y1 - y2) <= 1 and abs(x1 - x2) <= 1\n",
        "\n",
        "    def process_array(self):\n",
        "        height, width = self.array.shape\n",
        "        valid_twos = []\n",
        "        \n",
        "        # Step 1: Find all valid 2's\n",
        "        for y in range(height):\n",
        "            for x in range(width):\n",
        "                \n",
        "                if self.array[y, x] == 2 and self.is_valid_two(y, x):\n",
        "                    valid_twos.append((y, x))\n",
        "        \n",
        "        # Step 2: Draw lines between closest pairs of valid 2's\n",
        "        while len(valid_twos) > 1:\n",
        "            temp = self.find_closest_pair(valid_twos)\n",
        "            if isinstance(temp, tuple):\n",
        "                p1, p2 = self.find_closest_pair(valid_twos)\n",
        "            else:\n",
        "                print(\"here\")\n",
        "                break\n",
        "            self.draw_line(p1, p2)\n",
        "            valid_twos.remove(p1)\n",
        "            valid_twos.remove(p2)\n",
        "\n",
        "        return self.array\n",
        "    \n",
        "    def turn_twos_to_zeros_no_diagonals(self):\n",
        "        \"\"\"\n",
        "        Turn all 2's that touch a 0 into a 0, considering only horizontal and vertical neighbors,\n",
        "        and repeat the process until no pixels are changed.\n",
        "        \"\"\"\n",
        "        height, width = self.array.shape\n",
        "        changed = True\n",
        "        \n",
        "        while changed:\n",
        "            changed = False\n",
        "            new_array = self.array.copy()\n",
        "            \n",
        "            for y in range(height):\n",
        "                for x in range(width):\n",
        "                    if self.array[y, x] == 2:\n",
        "                        # Check only 4 possible neighbors (up, down, left, right)\n",
        "                        for dy, dx in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "                            ny, nx = y + dy, x + dx\n",
        "                            if 0 <= ny < height and 0 <= nx < width:\n",
        "                                if self.array[ny, nx] == 0:\n",
        "                                    new_array[y, x] = 0\n",
        "                                    changed = True\n",
        "                                    break\n",
        "                                    \n",
        "            self.array = new_array\n",
        "\n",
        "        return self.array\n",
        "    def turn_remaining_twos_to_ones(self):\n",
        "        \"\"\"\n",
        "        Turn all remaining 2's into 1's.\n",
        "        \"\"\"\n",
        "        self.array[self.array == 2] = 1\n",
        "        return self.array\n",
        "\n",
        "\n",
        "estimator = pothole_estimator(np.array(predicted_mask))\n",
        "estimator.process_array()\n",
        "estimator.turn_twos_to_zeros_no_diagonals()\n",
        "final_array = estimator.turn_remaining_twos_to_ones()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "array_to_image(np.array(predicted_mask)).save(\"output.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "def calculate_distance(x1, y1, x2, y2):\n",
        "    return sqrt((x2 - x1) ** 2 + (y1 - y2) ** 2)\n",
        "\n",
        "def process_data(data):\n",
        "    records = []\n",
        "    \n",
        "    for image_name, group in data.groupby('image'):\n",
        "        points = group[['x', 'y']].values\n",
        "        \n",
        "        if len(points) == 2:\n",
        "            distance = calculate_distance(points[0][0], points[0][1], points[1][0], points[1][1])\n",
        "        elif len(points) == 3:\n",
        "            d1 = calculate_distance(points[0][0], points[0][1], points[1][0], points[1][1])\n",
        "            d2 = calculate_distance(points[0][0], points[0][1], points[2][0], points[2][1])\n",
        "            d3 = calculate_distance(points[1][0], points[1][1], points[2][0], points[2][1])\n",
        "            distance = np.mean(sorted([d1, d2, d3])[:2])\n",
        "        else:\n",
        "            distance = None  # Handle cases with more or less than 2 or 3 points\n",
        "        \n",
        "        records.append({'image': image_name, 'distance': distance})\n",
        "    \n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Replace 'data.csv' with your actual CSV file path\n",
        "csv_file_path = './test_cluster_centroids.csv'\n",
        "\n",
        "# Read the data from the CSV file\n",
        "df_test_cent = pd.read_csv(csv_file_path, header=0, names=['image', 'point', 'x', 'y'])\n",
        "\n",
        "# Ensure the x and y columns are floats\n",
        "df_test_cent['x'] = df_test_cent['x'].astype(float)\n",
        "df_test_cent['y'] = df_test_cent['y'].astype(float)\n",
        "df_test_cent['point'] = df_test_cent['point'].astype(int)\n",
        "\n",
        "# Replace 'data.csv' with your actual CSV file path\n",
        "csv_file_path = './cluster_centroids.csv'\n",
        "\n",
        "# Read the data from the CSV file\n",
        "df_train_cent = pd.read_csv(csv_file_path, header=0, names=['image', 'point', 'x', 'y'])\n",
        "\n",
        "# Ensure the x and y columns are floats\n",
        "df_train_cent ['x'] = df_train_cent ['x'].astype(float)\n",
        "df_train_cent ['y'] = df_train_cent ['y'].astype(float)\n",
        "df_train_cent ['point'] = df_train_cent ['point'].astype(int)\n",
        "\n",
        "\n",
        "train_dists = process_data(df_train_cent)\n",
        "test_dists = process_data(df_test_cent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done with p1291.jpg\n",
            "done with p2008.jpg\n",
            "done with p1256.jpg\n",
            "done with p1994.jpg\n",
            "done with p1341.jpg\n",
            "done with p1986.jpg\n",
            "done with p1234.jpg\n",
            "done with p1306.jpg\n",
            "here\n",
            "done with p1142.jpg\n",
            "done with p2004.jpg\n",
            "done with p1118.jpg\n",
            "done with p1412.jpg\n",
            "done with p1942.jpg\n",
            "done with p136.jpg\n",
            "done with p1425.jpg\n",
            "done with p1424.jpg\n",
            "done with p1242.jpg\n",
            "done with p1100.jpg\n",
            "done with p1193.jpg\n",
            "done with p431.jpg\n",
            "done with p1195.jpg\n",
            "done with p413.jpg\n",
            "done with p132.jpg\n",
            "done with p1112.jpg\n",
            "done with p1948.jpg\n",
            "done with p438.jpg\n",
            "done with p2029.jpg\n",
            "done with p1088.jpg\n",
            "done with p123.jpg\n",
            "done with p1171.jpg\n",
            "done with p1344.jpg\n",
            "done with p1258.jpg\n",
            "done with p1285.jpg\n",
            "done with p1140.jpg\n",
            "done with p412.jpg\n",
            "done with p469.jpg\n",
            "done with p1988.jpg\n",
            "done with p1925.jpg\n",
            "done with p1355.jpg\n",
            "done with p1060.jpg\n",
            "done with p1196.jpg\n",
            "done with p1335.jpg\n",
            "done with p122.jpg\n",
            "done with p1075.jpg\n",
            "done with p1056.jpg\n",
            "done with p1218.jpg\n",
            "done with p1323.jpg\n",
            "done with p1104.jpg\n",
            "done with p410.jpg\n",
            "done with p1358.jpg\n",
            "done with p1090.jpg\n",
            "done with p1987.jpg\n",
            "done with p1214.jpg\n",
            "here\n",
            "done with p1941.jpg\n",
            "done with p1966.jpg\n",
            "done with p426.jpg\n",
            "done with p1269.jpg\n",
            "done with p2001.jpg\n",
            "done with p1121.jpg\n",
            "done with p1238.jpg\n",
            "done with p1200.jpg\n",
            "done with p1194.jpg\n",
            "done with p407.jpg\n",
            "done with p1159.jpg\n",
            "done with p1197.jpg\n",
            "done with p1973.jpg\n",
            "done with p416.jpg\n",
            "done with p1082.jpg\n",
            "done with p1179.jpg\n",
            "done with p437.jpg\n",
            "done with p1203.jpg\n",
            "done with p1212.jpg\n",
            "done with p1199.jpg\n",
            "done with p472.jpg\n",
            "done with p1109.jpg\n",
            "done with p445.jpg\n",
            "done with p1076.jpg\n",
            "done with p1063.jpg\n",
            "done with p1407.jpg\n",
            "done with p101.jpg\n",
            "done with p1976.jpg\n",
            "here\n",
            "done with p2015.jpg\n",
            "done with p128.jpg\n",
            "done with p443.jpg\n",
            "done with p1328.jpg\n",
            "done with p1186.jpg\n",
            "done with p1035.jpg\n",
            "done with p2024.jpg\n",
            "done with p498.jpg\n",
            "done with p414.jpg\n",
            "done with p1992.jpg\n",
            "done with p1165.jpg\n",
            "done with p429.jpg\n",
            "done with p1059.jpg\n",
            "done with p1984.jpg\n",
            "done with p1414.jpg\n",
            "done with p1128.jpg\n",
            "done with p1226.jpg\n",
            "done with p150.jpg\n",
            "done with p499.jpg\n",
            "done with p1333.jpg\n",
            "done with p1073.jpg\n",
            "done with p1294.jpg\n",
            "done with p440.jpg\n",
            "done with p1318.jpg\n",
            "done with p1971.jpg\n",
            "done with p2022.jpg\n",
            "done with p127.jpg\n",
            "done with p467.jpg\n",
            "done with p442.jpg\n",
            "done with p139.jpg\n",
            "done with p1345.jpg\n",
            "done with p1336.jpg\n",
            "done with p1439.jpg\n",
            "done with p409.jpg\n",
            "done with p2012.jpg\n",
            "done with p1122.jpg\n",
            "done with p1346.jpg\n",
            "done with p1207.jpg\n",
            "done with p1427.jpg\n",
            "done with p1951.jpg\n",
            "done with p417.jpg\n",
            "done with p1202.jpg\n",
            "done with p2011.jpg\n",
            "done with p118.jpg\n",
            "done with p134.jpg\n",
            "done with p1429.jpg\n",
            "done with p1177.jpg\n",
            "done with p1089.jpg\n",
            "done with p1969.jpg\n",
            "done with p1127.jpg\n",
            "done with p1990.jpg\n",
            "done with p2019.jpg\n",
            "done with p1230.jpg\n",
            "done with p1183.jpg\n",
            "done with p2002.jpg\n",
            "done with p1960.jpg\n",
            "done with p1275.jpg\n",
            "done with p1277.jpg\n",
            "done with p449.jpg\n",
            "done with p112.jpg\n",
            "done with p435.jpg\n",
            "done with p1169.jpg\n",
            "done with p500.jpg\n",
            "done with p1055.jpg\n",
            "done with p1354.jpg\n",
            "done with p423.jpg\n",
            "done with p1160.jpg\n",
            "done with p124.jpg\n",
            "done with p1431.jpg\n",
            "done with p408.jpg\n",
            "done with p106.jpg\n",
            "done with p1157.jpg\n",
            "done with p1247.jpg\n",
            "done with p1445.jpg\n",
            "done with p430.jpg\n",
            "done with p107.jpg\n",
            "done with p1084.jpg\n",
            "done with p1213.jpg\n",
            "done with p1047.jpg\n",
            "done with p1042.jpg\n",
            "done with p421.jpg\n",
            "done with p1342.jpg\n",
            "done with p1288.jpg\n",
            "done with p455.jpg\n",
            "done with p1245.jpg\n",
            "done with p1283.jpg\n",
            "done with p468.jpg\n",
            "done with p1926.jpg\n",
            "done with p1235.jpg\n",
            "done with p439.jpg\n",
            "done with p411.jpg\n",
            "done with p1350.jpg\n",
            "done with p476.jpg\n",
            "done with p1108.jpg\n",
            "done with p1929.jpg\n",
            "done with p116.jpg\n",
            "done with p456.jpg\n",
            "done with p1272.jpg\n",
            "done with p1991.jpg\n",
            "done with p1227.jpg\n",
            "done with p1254.jpg\n",
            "done with p1105.jpg\n",
            "done with p1449.jpg\n",
            "done with p451.jpg\n",
            "done with p1408.jpg\n",
            "done with p129.jpg\n",
            "done with p1096.jpg\n",
            "done with p1302.jpg\n",
            "done with p1332.jpg\n",
            "done with p1061.jpg\n",
            "done with p1125.jpg\n",
            "done with p1204.jpg\n",
            "done with p1334.jpg\n",
            "done with p1080.jpg\n",
            "done with p1180.jpg\n",
            "done with p478.jpg\n",
            "done with p474.jpg\n",
            "done with p125.jpg\n",
            "done with p1034.jpg\n",
            "here\n",
            "done with p2003.jpg\n",
            "done with p425.jpg\n",
            "done with p1229.jpg\n",
            "done with p2000.jpg\n",
            "done with p149.jpg\n",
            "done with p135.jpg\n",
            "done with p1098.jpg\n",
            "done with p1349.jpg\n",
            "done with p141.jpg\n",
            "done with p2016.jpg\n",
            "done with p110.jpg\n",
            "done with p1210.jpg\n",
            "done with p1331.jpg\n",
            "done with p1069.jpg\n",
            "done with p1079.jpg\n",
            "done with p1131.jpg\n",
            "done with p1228.jpg\n",
            "done with p1303.jpg\n",
            "here\n",
            "done with p126.jpg\n",
            "done with p119.jpg\n",
            "done with p1137.jpg\n",
            "done with p1955.jpg\n",
            "done with p441.jpg\n",
            "done with p1232.jpg\n",
            "done with p2014.jpg\n",
            "done with p1997.jpg\n",
            "done with p1932.jpg\n",
            "done with p1148.jpg\n",
            "done with p1930.jpg\n",
            "done with p1305.jpg\n",
            "done with p1326.jpg\n",
            "done with p1208.jpg\n",
            "done with p1963.jpg\n",
            "done with p1120.jpg\n",
            "done with p1184.jpg\n",
            "done with p1155.jpg\n",
            "done with p1962.jpg\n",
            "done with p1154.jpg\n",
            "here\n",
            "done with p1038.jpg\n",
            "done with p1119.jpg\n",
            "done with p1216.jpg\n",
            "done with p1287.jpg\n",
            "done with p1240.jpg\n",
            "done with p405.jpg\n",
            "done with p422.jpg\n",
            "done with p1083.jpg\n",
            "done with p471.jpg\n",
            "done with p1170.jpg\n",
            "done with p1998.jpg\n",
            "done with p117.jpg\n",
            "done with p1931.jpg\n",
            "done with p1957.jpg\n",
            "done with p1989.jpg\n",
            "done with p1201.jpg\n",
            "done with p1961.jpg\n",
            "done with p452.jpg\n",
            "done with p1116.jpg\n",
            "done with p1070.jpg\n",
            "done with p121.jpg\n",
            "done with p1147.jpg\n",
            "done with p1113.jpg\n",
            "done with p1264.jpg\n",
            "done with p477.jpg\n",
            "done with p466.jpg\n",
            "done with p1103.jpg\n",
            "done with p1298.jpg\n",
            "done with p102.jpg\n",
            "done with p475.jpg\n",
            "done with p1946.jpg\n",
            "done with p454.jpg\n",
            "done with p1188.jpg\n",
            "done with p1965.jpg\n",
            "done with p1077.jpg\n",
            "done with p1266.jpg\n",
            "done with p1036.jpg\n",
            "done with p446.jpg\n",
            "done with p1304.jpg\n",
            "done with p1174.jpg\n",
            "done with p1433.jpg\n",
            "done with p2027.jpg\n",
            "done with p1092.jpg\n",
            "done with p1049.jpg\n",
            "done with p1052.jpg\n",
            "here\n",
            "done with p1981.jpg\n",
            "done with p497.jpg\n",
            "done with p2009.jpg\n",
            "done with p433.jpg\n",
            "here\n",
            "done with p1321.jpg\n",
            "done with p428.jpg\n",
            "done with p109.jpg\n",
            "done with p1310.jpg\n",
            "done with p1231.jpg\n",
            "done with p1048.jpg\n",
            "done with p1443.jpg\n",
            "done with p1130.jpg\n",
            "done with p1106.jpg\n",
            "done with p1078.jpg\n",
            "done with p462.jpg\n",
            "done with p2013.jpg\n",
            "done with p1982.jpg\n",
            "done with p1442.jpg\n",
            "done with p1221.jpg\n",
            "done with p1450.jpg\n",
            "done with p1143.jpg\n",
            "done with p138.jpg\n",
            "done with p1190.jpg\n",
            "done with p1043.jpg\n",
            "done with p1282.jpg\n",
            "here\n",
            "done with p1940.jpg\n",
            "done with p1071.jpg\n",
            "done with p1101.jpg\n",
            "done with p1428.jpg\n",
            "done with p1133.jpg\n",
            "done with p1977.jpg\n",
            "done with p147.jpg\n",
            "done with p1952.jpg\n",
            "done with p1309.jpg\n",
            "done with p404.jpg\n",
            "done with p1312.jpg\n",
            "done with p1263.jpg\n",
            "done with p1091.jpg\n",
            "done with p1985.jpg\n",
            "done with p1135.jpg\n",
            "done with p1225.jpg\n",
            "done with p1923.jpg\n",
            "done with p453.jpg\n",
            "done with p1107.jpg\n",
            "done with p424.jpg\n",
            "done with p1149.jpg\n",
            "done with p1074.jpg\n",
            "done with p1239.jpg\n",
            "done with p1356.jpg\n",
            "done with p1284.jpg\n",
            "done with p1065.jpg\n",
            "done with p1248.jpg\n",
            "done with p1041.jpg\n",
            "done with p2021.jpg\n",
            "done with p1410.jpg\n",
            "done with p1224.jpg\n",
            "done with p1983.jpg\n",
            "done with p1972.jpg\n",
            "done with p1064.jpg\n",
            "here\n",
            "done with p1178.jpg\n",
            "done with p1440.jpg\n",
            "done with p1979.jpg\n",
            "done with p1097.jpg\n",
            "done with p1168.jpg\n",
            "done with p1236.jpg\n",
            "done with p1246.jpg\n",
            "done with p1253.jpg\n",
            "done with p1102.jpg\n",
            "done with p1144.jpg\n",
            "done with p120.jpg\n",
            "done with p415.jpg\n",
            "done with p1192.jpg\n",
            "done with p1413.jpg\n",
            "done with p1347.jpg\n",
            "done with p1251.jpg\n",
            "here\n",
            "done with p418.jpg\n",
            "done with p1276.jpg\n",
            "done with p444.jpg\n",
            "done with p1032.jpg\n",
            "done with p1293.jpg\n",
            "done with p1995.jpg\n",
            "done with p1271.jpg\n",
            "done with p2026.jpg\n",
            "done with p1968.jpg\n",
            "done with p1274.jpg\n",
            "done with p1126.jpg\n",
            "done with p1257.jpg\n",
            "done with p1420.jpg\n",
            "done with p140.jpg\n",
            "here\n",
            "done with p1974.jpg\n",
            "done with p465.jpg\n",
            "done with p1928.jpg\n",
            "done with p1138.jpg\n",
            "done with p1297.jpg\n",
            "done with p1111.jpg\n",
            "done with p447.jpg\n",
            "done with p1185.jpg\n",
            "done with p1087.jpg\n",
            "done with p2020.jpg\n",
            "done with p1352.jpg\n",
            "done with p2023.jpg\n",
            "done with p1286.jpg\n",
            "done with p1158.jpg\n",
            "done with p1081.jpg\n",
            "done with p1949.jpg\n",
            "done with p1114.jpg\n",
            "done with p145.jpg\n",
            "done with p1054.jpg\n",
            "done with p1980.jpg\n",
            "here\n",
            "done with p1978.jpg\n",
            "done with p1308.jpg\n",
            "done with p1219.jpg\n",
            "done with p1295.jpg\n",
            "done with p1970.jpg\n",
            "done with p1944.jpg\n",
            "done with p1935.jpg\n",
            "here\n",
            "done with p1037.jpg\n",
            "done with p1187.jpg\n",
            "done with p463.jpg\n",
            "done with p1325.jpg\n",
            "done with p432.jpg\n",
            "done with p146.jpg\n",
            "done with p2017.jpg\n",
            "here\n",
            "done with p448.jpg\n",
            "done with p1217.jpg\n",
            "done with p1959.jpg\n",
            "done with p1172.jpg\n",
            "done with p1062.jpg\n",
            "done with p1950.jpg\n",
            "done with p2005.jpg\n",
            "done with p1939.jpg\n",
            "done with p1417.jpg\n",
            "done with p113.jpg\n",
            "done with p1953.jpg\n",
            "done with p1141.jpg\n",
            "done with p1320.jpg\n",
            "done with p1934.jpg\n",
            "done with p1316.jpg\n",
            "done with p1314.jpg\n",
            "done with p1260.jpg\n",
            "done with p111.jpg\n",
            "done with p1182.jpg\n",
            "done with p1343.jpg\n",
            "done with p1095.jpg\n",
            "done with p1072.jpg\n",
            "done with p1993.jpg\n",
            "done with p1324.jpg\n",
            "done with p1418.jpg\n",
            "done with p1058.jpg\n",
            "done with p1259.jpg\n",
            "done with p1281.jpg\n",
            "done with p1406.jpg\n",
            "done with p420.jpg\n",
            "done with p1051.jpg\n",
            "done with p461.jpg\n",
            "done with p1123.jpg\n",
            "done with p2018.jpg\n",
            "done with p1956.jpg\n",
            "done with p1117.jpg\n",
            "done with p1357.jpg\n",
            "done with p2028.jpg\n",
            "done with p1166.jpg\n",
            "done with p1337.jpg\n",
            "done with p1099.jpg\n",
            "done with p1153.jpg\n",
            "done with p142.jpg\n",
            "done with p1416.jpg\n",
            "done with p1313.jpg\n",
            "done with p1943.jpg\n",
            "done with p115.jpg\n",
            "here\n",
            "done with p1267.jpg\n",
            "done with p1167.jpg\n",
            "done with p1033.jpg\n",
            "done with p1139.jpg\n",
            "done with p133.jpg\n",
            "done with p1189.jpg\n",
            "done with p1936.jpg\n",
            "done with p1173.jpg\n",
            "done with p1164.jpg\n",
            "done with p1110.jpg\n",
            "done with p148.jpg\n",
            "done with p419.jpg\n",
            "done with p1156.jpg\n",
            "done with p427.jpg\n",
            "done with p1176.jpg\n",
            "done with p1999.jpg\n",
            "done with p1315.jpg\n",
            "done with p1243.jpg\n",
            "done with p1268.jpg\n",
            "done with p1292.jpg\n",
            "done with p1039.jpg\n",
            "done with p2006.jpg\n",
            "done with p1945.jpg\n",
            "done with p1129.jpg\n",
            "done with p2007.jpg\n",
            "done with p1265.jpg\n",
            "done with p1233.jpg\n",
            "done with p1299.jpg\n",
            "done with p460.jpg\n",
            "done with p114.jpg\n",
            "done with p144.jpg\n",
            "here\n",
            "done with p473.jpg\n",
            "done with p104.jpg\n",
            "done with p1198.jpg\n",
            "done with p479.jpg\n",
            "done with p1040.jpg\n",
            "done with p450.jpg\n",
            "done with p1278.jpg\n",
            "done with p1409.jpg\n",
            "done with p1280.jpg\n",
            "done with p1115.jpg\n",
            "done with p406.jpg\n",
            "done with p1250.jpg\n",
            "done with p1270.jpg\n",
            "done with p1162.jpg\n",
            "done with p1086.jpg\n",
            "done with p1181.jpg\n",
            "done with p1296.jpg\n",
            "done with p105.jpg\n",
            "done with p108.jpg\n",
            "done with p434.jpg\n",
            "done with p1161.jpg\n",
            "done with p1438.jpg\n",
            "done with p1134.jpg\n",
            "done with p1205.jpg\n",
            "done with p470.jpg\n",
            "done with p143.jpg\n",
            "done with p1430.jpg\n",
            "done with p103.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "\n",
        "def count_ones(matrix):\n",
        "    count = np.count_nonzero(matrix == 1)\n",
        "    \n",
        "    return count\n",
        "\n",
        "def process_image(image):\n",
        "    current_image = test_transform(image).cuda().view(1, 3, 448, 448)\n",
        "    final_model.eval()\n",
        "    out = final_model(current_image)[\"out\"]\n",
        "    predicted_mask = torch.argmax(out, dim=1).view(448, 448).cpu()  \n",
        "    estimator = pothole_estimator(np.array(predicted_mask))\n",
        "    estimator.process_array()\n",
        "    estimator.turn_twos_to_zeros_no_diagonals()\n",
        "    final_array = estimator.turn_remaining_twos_to_ones()\n",
        "    area = count_ones(final_array)\n",
        "    \n",
        "    return area\n",
        "\n",
        "def process_images_in_directory(directory, output_csv):\n",
        "    # Open CSV file for writing\n",
        "    with open(output_csv, 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        # Write the header row\n",
        "        csvwriter.writerow(['Filename', 'Result'])\n",
        "\n",
        "        # Iterate over all files in the directory\n",
        "        for filename in os.listdir(directory):\n",
        "            \n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')):\n",
        "                print(f\"done with {filename}\")\n",
        "                filepath = os.path.join(directory, filename)\n",
        "                \n",
        "                # Open the image\n",
        "                with Image.open(filepath) as img:\n",
        "                    # Process the image\n",
        "                    img = img.convert(\"RGB\")\n",
        "                    result = process_image(img)\n",
        "                    \n",
        "                    # Write the result to the CSV file\n",
        "                    csvwriter.writerow([filename, result])\n",
        "\n",
        "\n",
        "image_directory = './data/train_images_segmented/'\n",
        "output_csv_file = './train_areas.csv'\n",
        "process_images_in_directory(image_directory, output_csv_file)\n",
        "\n",
        "image_directory = './data/test set resized/'\n",
        "output_csv_file = './test_areas.csv'\n",
        "process_images_in_directory(image_directory, output_csv_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bags used</th>\n",
              "      <th>Pothole</th>\n",
              "      <th>Result</th>\n",
              "      <th>distance</th>\n",
              "      <th>area per pixel</th>\n",
              "      <th>real area (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50</td>\n",
              "      <td>101</td>\n",
              "      <td>63031</td>\n",
              "      <td>213.483484</td>\n",
              "      <td>0.054854</td>\n",
              "      <td>3457.527494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00</td>\n",
              "      <td>102</td>\n",
              "      <td>33979</td>\n",
              "      <td>147.500671</td>\n",
              "      <td>0.114908</td>\n",
              "      <td>3904.474652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.50</td>\n",
              "      <td>106</td>\n",
              "      <td>43611</td>\n",
              "      <td>200.223877</td>\n",
              "      <td>0.062360</td>\n",
              "      <td>2719.595535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.50</td>\n",
              "      <td>107</td>\n",
              "      <td>19125</td>\n",
              "      <td>157.551512</td>\n",
              "      <td>0.100715</td>\n",
              "      <td>1926.177488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>109</td>\n",
              "      <td>40528</td>\n",
              "      <td>131.735646</td>\n",
              "      <td>0.144057</td>\n",
              "      <td>5838.329081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1442</td>\n",
              "      <td>58845</td>\n",
              "      <td>211.170435</td>\n",
              "      <td>0.056063</td>\n",
              "      <td>3299.007824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>0.25</td>\n",
              "      <td>1443</td>\n",
              "      <td>62997</td>\n",
              "      <td>318.353729</td>\n",
              "      <td>0.024667</td>\n",
              "      <td>1553.960564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1445</td>\n",
              "      <td>52918</td>\n",
              "      <td>311.229465</td>\n",
              "      <td>0.025809</td>\n",
              "      <td>1365.783991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1449</td>\n",
              "      <td>27550</td>\n",
              "      <td>228.270983</td>\n",
              "      <td>0.047978</td>\n",
              "      <td>1321.783099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>2.00</td>\n",
              "      <td>1450</td>\n",
              "      <td>82795</td>\n",
              "      <td>205.287625</td>\n",
              "      <td>0.059322</td>\n",
              "      <td>4911.550124</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>343 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Bags used   Pothole  Result    distance  area per pixel  real area (cm)\n",
              "0          0.50      101   63031  213.483484        0.054854     3457.527494\n",
              "1          1.00      102   33979  147.500671        0.114908     3904.474652\n",
              "2          0.50      106   43611  200.223877        0.062360     2719.595535\n",
              "3          0.50      107   19125  157.551512        0.100715     1926.177488\n",
              "4          0.50      109   40528  131.735646        0.144057     5838.329081\n",
              "..          ...      ...     ...         ...             ...             ...\n",
              "338        1.00     1442   58845  211.170435        0.056063     3299.007824\n",
              "339        0.25     1443   62997  318.353729        0.024667     1553.960564\n",
              "340        0.50     1445   52918  311.229465        0.025809     1365.783991\n",
              "341        0.50     1449   27550  228.270983        0.047978     1321.783099\n",
              "342        2.00     1450   82795  205.287625        0.059322     4911.550124\n",
              "\n",
              "[343 rows x 6 columns]"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_areas_df = pd.read_csv(header=0, filepath_or_buffer=\"./train_areas.csv\")\n",
        "train_areas_df[\"Pothole\"] = train_areas_df[\"Filename\"].apply(lambda x: int(x[1:-4]))\n",
        "train_areas_df = train_areas_df.drop(columns=\"Filename\")\n",
        "\n",
        "# train_dists[\"Pothole\"] = train_dists[\"image\"].apply(lambda x: int(x[6:-4]))\n",
        "# train_dists = train_dists.drop(columns = \"image\")\n",
        "\n",
        "real_area_train_df = pd.merge(train_areas_df, train_dists, on='Pothole', how='inner')\n",
        "\n",
        "real_area_train_df[\"area per pixel\"] = real_area_train_df[\"distance\"].apply(lambda x: 2500 / (x**2))\n",
        "real_area_train_df['real area (cm)'] = real_area_train_df[\"Result\"] * real_area_train_df[\"area per pixel\"]\n",
        "#real_area_train_df = real_area_train_df.drop(columns = [\"area per pixel\"])\n",
        "\n",
        "train_label_df = pd.read_csv(\"./data/train_labels.csv\", header=0)\n",
        "train_label_df[\"Pothole\"] = train_label_df[\"Pothole number\"]\n",
        "train_label_df = train_label_df.drop(columns = \"Pothole number\")\n",
        "\n",
        "df_train = pd.merge(train_label_df, real_area_train_df, on = \"Pothole\", how = \"inner\")\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_403656/3961350192.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train.drop(remove, inplace=True)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bags used</th>\n",
              "      <th>Pothole</th>\n",
              "      <th>real area (cm)</th>\n",
              "      <th>bags with noise</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50</td>\n",
              "      <td>101</td>\n",
              "      <td>3457.527494</td>\n",
              "      <td>0.563079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00</td>\n",
              "      <td>102</td>\n",
              "      <td>3904.474652</td>\n",
              "      <td>0.974498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.50</td>\n",
              "      <td>106</td>\n",
              "      <td>2719.595535</td>\n",
              "      <td>0.589893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.50</td>\n",
              "      <td>107</td>\n",
              "      <td>1926.177488</td>\n",
              "      <td>0.560965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>109</td>\n",
              "      <td>5838.329081</td>\n",
              "      <td>0.541364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1442</td>\n",
              "      <td>3299.007824</td>\n",
              "      <td>1.052861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>0.25</td>\n",
              "      <td>1443</td>\n",
              "      <td>1553.960564</td>\n",
              "      <td>0.310260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1445</td>\n",
              "      <td>1365.783991</td>\n",
              "      <td>0.492602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1449</td>\n",
              "      <td>1321.783099</td>\n",
              "      <td>0.452446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>2.00</td>\n",
              "      <td>1450</td>\n",
              "      <td>4911.550124</td>\n",
              "      <td>2.024923</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>337 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Bags used   Pothole  real area (cm)  bags with noise\n",
              "0          0.50      101     3457.527494         0.563079\n",
              "1          1.00      102     3904.474652         0.974498\n",
              "2          0.50      106     2719.595535         0.589893\n",
              "3          0.50      107     1926.177488         0.560965\n",
              "4          0.50      109     5838.329081         0.541364\n",
              "..          ...      ...             ...              ...\n",
              "338        1.00     1442     3299.007824         1.052861\n",
              "339        0.25     1443     1553.960564         0.310260\n",
              "340        0.50     1445     1365.783991         0.492602\n",
              "341        0.50     1449     1321.783099         0.452446\n",
              "342        2.00     1450     4911.550124         2.024923\n",
              "\n",
              "[337 rows x 4 columns]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remove = np.where((df_train[\"real area (cm)\"] > 12000))[0]\n",
        "\n",
        "mean = 0\n",
        "std_dev = 0.05\n",
        "df_train.drop(remove, inplace=True)\n",
        "df_train = df_train.dropna()\n",
        "noise = np.random.normal(mean, std_dev, size=df_train.shape[0])\n",
        "df_train[\"bags with noise\"] = df_train[\"Bags used \"] + noise\n",
        "plt.scatter(x = df_train[\"real area (cm)\"], y = df_train[\"bags with noise\"])\n",
        "plt.show()\n",
        "#df_train[\"real area (cm)\"].value_counts()\n",
        "X_log = np.sqrt(df_train[\"real area (cm)\"])\n",
        "X_power = df_train[\"real area (cm)\"] ** 0.3\n",
        "y_log = np.log(df_train[\"Bags used \"])\n",
        "\n",
        "#X_binned = pd.cut(np.array(df_train[\"real area (cm)\"]).flatten(), bins=[0, 1900, 2500, np.inf], labels=[0, 1, 2])\n",
        "\n",
        "df_train\n",
        "# plt.scatter(x = X_binned, y = df_train[\"Bags used \"] + noise)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "xx must be a list of numbers with minimum length 3",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[111], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df_train[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39marray(df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBags used \u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m three_fit \u001b[38;5;241m=\u001b[39m \u001b[43mpiecewise_regression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_breakpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m three_fit\u001b[38;5;241m.\u001b[39msummary()\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/piecewise_regression/main.py:674\u001b[0m, in \u001b[0;36mFit.__init__\u001b[0;34m(self, xx, yy, start_values, n_breakpoints, n_boot, verbose, max_iterations, tolerance, min_distance_between_breakpoints, min_distance_to_edge)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstantiating Fit . . . \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    673\u001b[0m \u001b[38;5;66;03m# Validate all input data\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxx \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_list_of_numbers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myy \u001b[38;5;241m=\u001b[39m validate_list_of_numbers(yy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myy\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myy) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxx):\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/piecewise_regression/data_validation.py:64\u001b[0m, in \u001b[0;36mvalidate_list_of_numbers\u001b[0;34m(var, var_name, min_length)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(value_error_text)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(var) \u001b[38;5;241m<\u001b[39m min_length:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(value_error_text)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m var\n",
            "\u001b[0;31mValueError\u001b[0m: xx must be a list of numbers with minimum length 3"
          ]
        }
      ],
      "source": [
        "import piecewise_regression\n",
        "X_train = np.array(df_train[[\"distance\", \"Result\"]]).reshape(1, -1)\n",
        "y =  np.array(df_train[\"Bags used \"]).reshape(1, -1)\n",
        "three_fit = piecewise_regression.Fit(X_train,y, n_breakpoints=3)\n",
        "three_fit.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pw_fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "X = np.array(df_train[\"real area (cm)\"]).reshape(-1, 1)\n",
        "y = df_train[\"Bags used \"]\n",
        "linear_model = LinearRegression()\n",
        "df_train.dropna(inplace=True)\n",
        "distance_square = df_train[\"distance\"]**2\n",
        "linear_model.fit(df_train[['distance', \"area per pixel\"]], df_train[\"Bags used \"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.45565987999295765"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear_model.score(df_train[['distance', \"area per pixel\"]], df_train[\"Bags used \"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.to_csv(\"dataframe.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_model = SVC(kernel='linear')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_areas_df = pd.read_csv(header=0, filepath_or_buffer=\"./test_areas.csv\")\n",
        "test_areas_df[\"Pothole\"] = test_areas_df[\"Filename\"].apply(lambda x: int(x[1:-4]))\n",
        "test_areas_df = test_areas_df.drop(columns=\"Filename\")\n",
        "\n",
        "test_dists[\"Pothole\"] = test_dists[\"image\"].apply(lambda x: int(x[6:-4]))\n",
        "test_dist = test_dists.drop(columns = \"image\")\n",
        "\n",
        "test_real_area_df = pd.merge(test_areas_df, test_dist, on='Pothole', how='inner')\n",
        "\n",
        "test_real_area_df[\"area per pixel\"] = test_real_area_df[\"distance\"].apply(lambda x: 2500 / (x**2))\n",
        "test_real_area_df\n",
        "test_real_area_df['real area (cm)'] = test_real_area_df[\"Result\"] * test_real_area_df[\"area per pixel\"]\n",
        "#test_real_area_df = test_real_area_df.drop(columns = [\"area per pixel\"])\n",
        "\n",
        "X_test = test_real_area_df\n",
        "X_test.to_csv(\"test_dataframe.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test[\"predictions\"] = pd.Series(linear_model.predict(np.array(X_test[[\"distance\", \"area per pixel\"]])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction_df = pd.DataFrame(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction_df.loc[:, [\"Pothole\", \"predictions\"]].to_csv(\"submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_sorted = test_real_area_df.sort_values(by=\"Pothole\")\n",
        "X_test_sorted.loc[:, [\"Pothole\", \"predictions\"]].to_csv(\"new_sub.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "df = pd.read_csv(\"./dataframe.csv\")\n",
        "# Load your data (Here, I'm using the Iris dataset as an ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "X has 1 features, but LinearRegression is expecting 2 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[125], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mlinear_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/sklearn/linear_model/_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/sklearn/linear_model/_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/sklearn/base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.8/site-packages/sklearn/base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: X has 1 features, but LinearRegression is expecting 2 features as input."
          ]
        }
      ],
      "source": [
        "predictions = pd.DataFrame(linear_model.predict(np.array(X_test).reshape(-1, 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test[[\"Pothole\", \"predictions\"]].to_csv(\"last.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 ðŸš€ v7.0-353-g5eca7b9c Python-3.8.19 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce RTX 2060, 5918MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "#torch.hub.load('/opt/ml/model/code/', 'custom', source ='local', path='best.pt',force_reload=True)\n",
        "import torch\n",
        "depth_model=torch.hub.load('/home/johan/DataScience/yolo/yolov5','custom',path='./best.pt',source='local')\n",
        "#model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/johan/DataScience/yolo/yolov5/models/common.py:869: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        }
      ],
      "source": [
        "result = depth_model(\"data/train_images_segmented/p101.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "YOLOv5 <class 'models.common.Detections'> instance\n",
              "image 1/1: 562x562 (no detections)\n",
              "Speed: 8.3ms pre-process, 5.3ms inference, 9.0ms NMS per image at shape (1, 3, 640, 640)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
