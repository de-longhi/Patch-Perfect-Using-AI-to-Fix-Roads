{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i-ebO8XbEsEd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion() # (%matplotlib inline)\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy0o8aeIEsEf"
      },
      "source": [
        "# Patch Perfect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC__VfxoEsEi"
      },
      "source": [
        "This notebook will document the process we underwent to find a solution to the plothole-problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZOHtzy0EsEj"
      },
      "source": [
        "\n",
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY7iO0TbEsEj"
      },
      "source": [
        "We start by looking at the data systematically to see where we will inevitably need to solve problems before we create a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hVhsQ812EsEk"
      },
      "outputs": [],
      "source": [
        "# __file__ = os.path.abspath('') # notebooks are stupid\n",
        "# DATA_DIR = Path(__file__).resolve() / \"data\"\n",
        "# TRAIN_LABELS_PATH = DATA_DIR / \"train_labels.csv\"\n",
        "\n",
        "# train_label_df = pd.read_csv(filepath_or_buffer=TRAIN_LABELS_PATH)\n",
        "# train_label_df.rename(columns={'Bags used ': 'Bags used'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HfVHP5EDEsEl",
        "outputId": "81635175-1a4b-4146-cdea-b6fac2347de5"
      },
      "outputs": [],
      "source": [
        "# values = train_label_df.loc[:, 'Bags used'].value_counts()\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.hist(values, bins=range(1, max(values) + 2), edgecolor='black')\n",
        "# plt.title('Histogram of Data Points per Bag Amount')\n",
        "# plt.xlabel('Number of Data Points for Each Bag Amount')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMcpWCujEsEn"
      },
      "source": [
        "We can see that there is a massive class imbalance. This could create issues where a model trained on this dataset has a bias towards more common bags. Most values are between 0 and 1 with some values much higher. There are many strategies we could use to solve this, including but not limited to:\n",
        "<ul>\n",
        "<li>Some label abstraction technique where we might create labels based on a logarithmic scale</li>\n",
        "<li>Data augmentation as a class imbalance mitigation: This process is called upsampling</li>\n",
        "</ul>\n",
        "\n",
        "We should also consider the following: The data makes this problem seem like a regression model is needed, but tuning the labels may enable us to change it to a much simpler classification task at the cost of some accuracy. Doing this would result in a much more robust model and enable us to use techniques like label smoothing to let the model generalize more to unseen data.\n",
        "<hr>\n",
        "References:\n",
        "<ul>\n",
        "<li>Paperspace Blog. (2022). Data Augmentation: A Class Imbalance Mitigative Measure. [online] Available at: https://blog.paperspace.com/data-augmentation-a-class-imbalance-mitigative-measure/.</li>\n",
        "<li>S. Wang and X. Yao, \"Multiclass Imbalance Problems: Analysis and Potential Solutions,\" in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 42, no. 4, pp. 1119-1130, Aug. 2012, doi: 10.1109/TSMCB.2012.2187280. keywords: {Training;Correlation;Training data;Pattern analysis;Genetic algorithms;IEEE Potentials;Cybernetics;Boosting;diversity;ensemble learning;multiclass imbalance problems;negative correlation learning}, </li>\n",
        "</ul>\n",
        "‌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsDHOvH0EsEp"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kqPHOll3EsEq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_583706/3933020813.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(\"./pretrained_model.pt\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "model = torch.load(\"./pretrained_model.pt\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "      param.requires_grad = False\n",
        "\n",
        "\n",
        "model.classifier = nn.Sequential( # Change only the classifier of the model, I.E the last few layers\n",
        "\n",
        "    nn.Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "    nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1, inplace=False),\n",
        "    nn.Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1)) # Change the output to 3 classes instead of 21\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s-gk4B3EsEr",
        "outputId": "5ea90a64-eca6-41fd-bc95-9272a1567577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable layers: 75\n",
            "Non-trainable layers: 87\n"
          ]
        }
      ],
      "source": [
        "trainable_layers = 0\n",
        "non_trainable_layers = 0\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    # Check if any parameter in the layer requires gradients\n",
        "    if any(param.requires_grad for param in module.parameters()):\n",
        "        trainable_layers += 1\n",
        "    else:\n",
        "        non_trainable_layers += 1\n",
        "\n",
        "print(f\"Trainable layers: {trainable_layers}\")\n",
        "print(f\"Non-trainable layers: {non_trainable_layers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoRmM5y3EsEr",
        "outputId": "322824e9-04f0-4f38-cd24-5baf884d4369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 9485187\n",
            "Non-trainable parameters: 25827797\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trainable_params = 0\n",
        "non_trainable_params = 0\n",
        "\n",
        "for param in model.parameters():\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()  # Count the number of elements\n",
        "    else:\n",
        "        non_trainable_params += param.numel()\n",
        "\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Non-trainable parameters: {non_trainable_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSg-1m7EsEt"
      },
      "source": [
        "## Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ns4tJ9HoEsEt"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "# optimizer = optim.Adam(\n",
        "#     filter(lambda p: p.requires_grad, model.parameters()),\n",
        "#     lr=0.001\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=1):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = 3e-4) # Karpathy's number\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            # Move data to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.type(torch.LongTensor).cuda()\n",
        "                model.cuda()\n",
        "\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)['out']\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #running_loss += loss.item() * images.size(0)\n",
        "        test_loss = evaluate_model(model, val_loader)\n",
        "        #epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Accuracy: {test_loss:.4f}')\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            total_pixels += labels.numel()\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "\n",
        "        accuracy = total_correct / total_pixels\n",
        "        return accuracy\n",
        "        #print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmklZldlEsEu"
      },
      "source": [
        "## Data Prep Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ClR6aVHHEsEu"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class for your data\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "rgb_to_class = {\n",
        "    (0, 0, 0): 0,\n",
        "    (255, 255, 255): 1,\n",
        "    (100, 100, 100): 2\n",
        "}\n",
        "\n",
        "class JointTransform:\n",
        "    def __init__(self, image_transforms=None, mask_transforms=None):\n",
        "        self.image_transforms = image_transforms\n",
        "        self.mask_transforms = mask_transforms\n",
        "    @staticmethod\n",
        "    def set_seed(seed):\n",
        "      torch.manual_seed(seed)\n",
        "      torch.cuda.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "      random.seed(seed)\n",
        "      np.random.seed(seed)\n",
        "\n",
        "      # Ensure that all operations are deterministic\n",
        "      torch.backends.cudnn.deterministic = True\n",
        "      torch.backends.cudnn.benchmark = False\n",
        "    def __call__(self, image, mask):\n",
        "        if self.image_transforms:\n",
        "            seed = random.randint(0, 2**32)\n",
        "            self.set_seed(seed)\n",
        "            image = self.image_transforms(image)\n",
        "\n",
        "            self.set_seed(seed)\n",
        "            mask = self.mask_transforms(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "image_augmentations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "mask_augmentations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "# Instantiate joint transform\n",
        "augmentation = JointTransform(image_transforms=image_augmentations, mask_transforms=mask_augmentations)\n",
        "\n",
        "\n",
        "class Potholes(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, do_augmentation = True):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_filenames = os.listdir(image_dir)\n",
        "        self.mean = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "        self.do_augmentation = do_augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    @staticmethod\n",
        "    def rgb_to_mask(mask):\n",
        "        # Convert the mask to a numpy array\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # Initialize an array to hold the class indices\n",
        "        class_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)\n",
        "\n",
        "        # Apply the mapping from RGB values to class indices\n",
        "        for rgb, class_index in rgb_to_class.items():\n",
        "            matches = np.all(mask == rgb, axis=-1)\n",
        "            class_mask[matches] = class_index\n",
        "\n",
        "        return class_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, (img_name[:-4] + \"_mask.png\"))\n",
        "\n",
        "        # Load image and label\n",
        "        image = Image.open(img_path)\n",
        "        mask = Image.open(label_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply augmentations then transformations\n",
        "\n",
        "        if self.do_augmentation: image, mask = augmentation(image, mask)\n",
        "\n",
        "        image, mask = self.image_transforms(image, mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Define transforms for data augmentation and normalization\n",
        "\n",
        "    def image_transforms(self, image, label):\n",
        "        transform_images = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Resize to the desired input size\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "        transform_labels = transforms.Compose([\n",
        "            transforms.Resize((256, 256))\n",
        "        ])\n",
        "        mask = self.rgb_to_mask(transform_labels(label))\n",
        "        return transform_images(image), mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Paths to your dataset\n",
        "train_image_dir = \"./data/train_images_segmented/\"\n",
        "train_label_dir = \"./data/train_masks_segmented/\"\n",
        "\n",
        "val_image_dir = \"./data/validation set/\"\n",
        "val_label_dir = \"./data/validation masks/\"\n",
        "\n",
        "# size of stick/image indicates camera zoom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "gkGh36k8EsEu",
        "outputId": "a51b5930-8b0e-446c-fb47-b51823ad840f"
      },
      "outputs": [],
      "source": [
        "# Create datasets and data loaders\n",
        "train_dataset = Potholes(train_image_dir, train_label_dir, do_augmentation=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "val_dataset = Potholes(val_image_dir, val_label_dir, do_augmentation=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "#train_model(model = model,train_loader = train_loader,num_epochs= 1000, val_loader=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ExDTpR72fYNV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0):\n",
        "    model.to(\"cuda\")\n",
        "    number_in_epoch = len(train_loader) - 1\n",
        "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
        "    lr = init_value\n",
        "    optimizer.param_groups[0][\"lr\"] = lr\n",
        "    best_loss = float('inf')\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        batch_num += 1\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs, labels\n",
        "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)[\"out\"]\n",
        "        loss = loss_fn(outputs, labels.type(torch.LongTensor).cuda())\n",
        "\n",
        "        # Convert loss to float for comparison and storage\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        # Crash out if loss explodes\n",
        "        if batch_num > 1 and loss_value > 4 * best_loss:\n",
        "            return log_lrs[10:-5], losses[10:-5]\n",
        "\n",
        "        # Record the best loss\n",
        "        if loss_value < best_loss:\n",
        "            best_loss = loss_value\n",
        "\n",
        "        # Store the values\n",
        "        losses.append(loss_value)\n",
        "        log_lrs.append(math.log10(lr))\n",
        "\n",
        "        # Do the backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the lr for the next step and store\n",
        "        lr *= update_step\n",
        "        optimizer.param_groups[0][\"lr\"] = lr\n",
        "\n",
        "    return log_lrs[10:-5], losses[10:-5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#logs,losses = find_lr(model, nn.CrossEntropyLoss(), optim.Adam(model.parameters(), lr = 3e-4), train_loader=train_loader)\n",
        "#plt.plot(logs,losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_583706/1325712498.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  final_model = torch.load(\"./trained_model.pt\")\n"
          ]
        }
      ],
      "source": [
        "final_model = torch.load(\"./trained_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "test_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "test_image = Image.open(\"./data/stick subset/p429.jpg\").convert(\"RGB\")\n",
        "test_image = test_transform(test_image).cuda().view(1, 3, 448, 448)\n",
        "final_model.eval()\n",
        "out = final_model(test_image)[\"out\"]\n",
        "predicted_mask = torch.argmax(out, dim=1).view(448, 448).cpu()\n",
        "\n",
        "\n",
        "def array_to_image(array):\n",
        "    # Define the mapping from array values to RGB colors\n",
        "    color_mapping = {\n",
        "        0: (0, 0, 0),       # Black\n",
        "        1: (255, 255, 255), # White\n",
        "        2: (128, 128, 128)  # Gray\n",
        "    }\n",
        "    \n",
        "    # Convert the array to an RGB image\n",
        "    height, width = array.shape\n",
        "    image = Image.new('RGB', (width, height))\n",
        "    \n",
        "    for y in range(height):\n",
        "        for x in range(width):\n",
        "            image.putpixel((x, y), color_mapping[array[y, x]])\n",
        "    \n",
        "    return image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "class pothole_estimator():\n",
        "    def __init__(self, array):\n",
        "        self.array = array\n",
        "        \n",
        "    def is_valid_two(self, y, x):\n",
        "        \"\"\"Check if the element at (y, x) is a 2 with both 0 and 1 as neighbors.\"\"\"\n",
        "        height, width = self.array.shape\n",
        "        neighbors = []\n",
        "        \n",
        "        # Check all 8 possible neighbors\n",
        "        for dy in [-1, 0, 1]:\n",
        "            for dx in [-1, 0, 1]:\n",
        "                if dy == 0 and dx == 0:\n",
        "                    continue\n",
        "                ny, nx = y + dy, x + dx\n",
        "                if 0 <= ny < height and 0 <= nx < width:\n",
        "                    neighbors.append(self.array[ny, nx])\n",
        "        \n",
        "        return 0 in neighbors and 1 in neighbors\n",
        "    \n",
        "\n",
        "    def find_closest_pair(self, coords):\n",
        "        \"\"\"Find the pair of points in coords that are closest to each other.\"\"\"\n",
        "        min_dist = float('inf')\n",
        "        closest_pair = None\n",
        "        \n",
        "        for i in range(len(coords)):\n",
        "            for j in range(i + 1, len(coords)):\n",
        "                if self.is_adjacent(coords[i], coords[j]):\n",
        "                    continue\n",
        "                dist = np.linalg.norm(np.array(coords[i]) - np.array(coords[j]))\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    closest_pair = (coords[i], coords[j])\n",
        "    \n",
        "        return closest_pair\n",
        "        \"\"\"Draw a line of 0's between points p1 and p2 using Bresenham's line algorithm.\"\"\"\n",
        "    def draw_line(self, p1, p2):\n",
        "        y1, x1 = p1\n",
        "        y2, x2 = p2\n",
        "        dy = abs(y2 - y1)\n",
        "        dx = abs(x2 - x1)\n",
        "        sy = 1 if y1 < y2 else -1\n",
        "        sx = 1 if x1 < x2 else -1\n",
        "        err = dx - dy\n",
        "\n",
        "        while True:\n",
        "            self.array[y1, x1] = 1\n",
        "            if (y1, x1) == (y2, x2):\n",
        "                break\n",
        "            e2 = 2 * err\n",
        "            if e2 > -dy:\n",
        "                err -= dy\n",
        "                x1 += sx\n",
        "            if e2 < dx:\n",
        "                err += dx\n",
        "                y1 += sy\n",
        "    @staticmethod        \n",
        "    def is_adjacent(p1, p2):\n",
        "        \"\"\"Check if two points p1 and p2 are adjacent (including diagonally).\"\"\"\n",
        "        y1, x1 = p1\n",
        "        y2, x2 = p2\n",
        "        return abs(y1 - y2) <= 1 and abs(x1 - x2) <= 1\n",
        "\n",
        "    def process_array(self):\n",
        "        height, width = self.array.shape\n",
        "        valid_twos = []\n",
        "        \n",
        "        # Step 1: Find all valid 2's\n",
        "        for y in range(height):\n",
        "            for x in range(width):\n",
        "                \n",
        "                if self.array[y, x] == 2 and self.is_valid_two(y, x):\n",
        "                    valid_twos.append((y, x))\n",
        "        \n",
        "        # Step 2: Draw lines between closest pairs of valid 2's\n",
        "        while len(valid_twos) > 1:\n",
        "            temp = self.find_closest_pair(valid_twos)\n",
        "            if isinstance(temp, tuple):\n",
        "                p1, p2 = self.find_closest_pair(valid_twos)\n",
        "            else:\n",
        "                print(\"here\")\n",
        "                break\n",
        "            self.draw_line(p1, p2)\n",
        "            valid_twos.remove(p1)\n",
        "            valid_twos.remove(p2)\n",
        "\n",
        "        return self.array\n",
        "    \n",
        "    def turn_twos_to_zeros_no_diagonals(self):\n",
        "        \"\"\"\n",
        "        Turn all 2's that touch a 0 into a 0, considering only horizontal and vertical neighbors,\n",
        "        and repeat the process until no pixels are changed.\n",
        "        \"\"\"\n",
        "        height, width = self.array.shape\n",
        "        changed = True\n",
        "        \n",
        "        while changed:\n",
        "            changed = False\n",
        "            new_array = self.array.copy()\n",
        "            \n",
        "            for y in range(height):\n",
        "                for x in range(width):\n",
        "                    if self.array[y, x] == 2:\n",
        "                        # Check only 4 possible neighbors (up, down, left, right)\n",
        "                        for dy, dx in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "                            ny, nx = y + dy, x + dx\n",
        "                            if 0 <= ny < height and 0 <= nx < width:\n",
        "                                if self.array[ny, nx] == 0:\n",
        "                                    new_array[y, x] = 0\n",
        "                                    changed = True\n",
        "                                    break\n",
        "                                    \n",
        "            self.array = new_array\n",
        "\n",
        "        return self.array\n",
        "    def turn_remaining_twos_to_ones(self):\n",
        "        \"\"\"\n",
        "        Turn all remaining 2's into 1's.\n",
        "        \"\"\"\n",
        "        self.array[self.array == 2] = 1\n",
        "        return self.array\n",
        "\n",
        "\n",
        "estimator = pothole_estimator(np.array(predicted_mask))\n",
        "estimator.process_array()\n",
        "estimator.turn_twos_to_zeros_no_diagonals()\n",
        "final_array = estimator.turn_remaining_twos_to_ones()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "array_to_image(np.array(predicted_mask)).save(\"output.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "def calculate_distance(x1, y1, x2, y2):\n",
        "    return sqrt((x2 - x1) ** 2 + (y1 - y2) ** 2)\n",
        "\n",
        "def process_data(data):\n",
        "    records = []\n",
        "    \n",
        "    for image_name, group in data.groupby('image'):\n",
        "        points = group[['x', 'y']].values\n",
        "        \n",
        "        if len(points) == 2:\n",
        "            distance = calculate_distance(points[0][0], points[0][1], points[1][0], points[1][1])\n",
        "        elif len(points) == 3:\n",
        "            d1 = calculate_distance(points[0][0], points[0][1], points[1][0], points[1][1])\n",
        "            d2 = calculate_distance(points[0][0], points[0][1], points[2][0], points[2][1])\n",
        "            d3 = calculate_distance(points[1][0], points[1][1], points[2][0], points[2][1])\n",
        "            distance = np.mean(sorted([d1, d2, d3])[:2])\n",
        "        else:\n",
        "            distance = None  # Handle cases with more or less than 2 or 3 points\n",
        "        \n",
        "        records.append({'image': image_name, 'distance': distance})\n",
        "    \n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Replace 'data.csv' with your actual CSV file path\n",
        "csv_file_path = './cluster_centroids.csv'\n",
        "\n",
        "# Read the data from the CSV file\n",
        "df = pd.read_csv(csv_file_path, header=0, names=['image', 'point', 'x', 'y'])\n",
        "\n",
        "# Ensure the x and y columns are floats\n",
        "df['x'] = df['x'].astype(float)\n",
        "df['y'] = df['y'].astype(float)\n",
        "df['point'] = df['point'].astype(int)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mask_p101.jpg</td>\n",
              "      <td>213.483484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mask_p102.jpg</td>\n",
              "      <td>147.500671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mask_p1032.jpg</td>\n",
              "      <td>273.707347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mask_p1033.jpg</td>\n",
              "      <td>350.026297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mask_p1034.jpg</td>\n",
              "      <td>204.457867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>mask_p474.jpg</td>\n",
              "      <td>189.116503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>mask_p475.jpg</td>\n",
              "      <td>160.006514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>mask_p476.jpg</td>\n",
              "      <td>183.234848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>mask_p477.jpg</td>\n",
              "      <td>180.734857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>mask_p478.jpg</td>\n",
              "      <td>169.417385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              image    distance\n",
              "0     mask_p101.jpg  213.483484\n",
              "1     mask_p102.jpg  147.500671\n",
              "2    mask_p1032.jpg  273.707347\n",
              "3    mask_p1033.jpg  350.026297\n",
              "4    mask_p1034.jpg  204.457867\n",
              "..              ...         ...\n",
              "423   mask_p474.jpg  189.116503\n",
              "424   mask_p475.jpg  160.006514\n",
              "425   mask_p476.jpg  183.234848\n",
              "426   mask_p477.jpg  180.734857\n",
              "427   mask_p478.jpg  169.417385\n",
              "\n",
              "[428 rows x 2 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Process the data to calculate distances\n",
        "result_df = process_data(df)\n",
        "\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done with p1291.jpg\n",
            "done with p2008.jpg\n",
            "done with p1256.jpg\n",
            "done with p1994.jpg\n",
            "done with p1341.jpg\n",
            "done with p1986.jpg\n",
            "done with p1234.jpg\n",
            "done with p1306.jpg\n",
            "done with p1142.jpg\n",
            "done with p2004.jpg\n",
            "done with p1118.jpg\n",
            "done with p1412.jpg\n",
            "done with p1942.jpg\n",
            "done with p1425.jpg\n",
            "done with p1424.jpg\n",
            "done with p1242.jpg\n",
            "done with p1100.jpg\n",
            "done with p1193.jpg\n",
            "done with p431.jpg\n",
            "done with p1195.jpg\n",
            "done with p413.jpg\n",
            "done with p132.jpg\n",
            "done with p1112.jpg\n",
            "done with p1948.jpg\n",
            "done with p2029.jpg\n",
            "done with p1088.jpg\n",
            "done with p123.jpg\n",
            "done with p1344.jpg\n",
            "done with p1258.jpg\n",
            "done with p1285.jpg\n",
            "done with p1140.jpg\n",
            "done with p412.jpg\n",
            "done with p469.jpg\n",
            "done with p1988.jpg\n",
            "done with p1925.jpg\n",
            "done with p1355.jpg\n",
            "done with p1060.jpg\n",
            "done with p1196.jpg\n",
            "done with p1335.jpg\n",
            "done with p1075.jpg\n",
            "done with p1056.jpg\n",
            "done with p1218.jpg\n",
            "done with p1104.jpg\n",
            "done with p410.jpg\n",
            "done with p1358.jpg\n",
            "done with p1090.jpg\n",
            "done with p1987.jpg\n",
            "done with p1214.jpg\n",
            "done with p1941.jpg\n",
            "done with p1966.jpg\n",
            "done with p426.jpg\n",
            "done with p1269.jpg\n",
            "done with p2001.jpg\n",
            "done with p1121.jpg\n",
            "done with p1200.jpg\n",
            "done with p1194.jpg\n",
            "done with p407.jpg\n",
            "done with p1159.jpg\n",
            "done with p1197.jpg\n",
            "done with p1973.jpg\n",
            "done with p416.jpg\n",
            "done with p1082.jpg\n",
            "done with p1179.jpg\n",
            "done with p437.jpg\n",
            "done with p1212.jpg\n",
            "done with p1199.jpg\n",
            "done with p472.jpg\n",
            "done with p1109.jpg\n",
            "done with p445.jpg\n",
            "done with p1076.jpg\n",
            "done with p1063.jpg\n",
            "done with p1407.jpg\n",
            "done with p101.jpg\n",
            "done with p2015.jpg\n",
            "done with p128.jpg\n",
            "done with p443.jpg\n",
            "done with p1328.jpg\n",
            "done with p1186.jpg\n",
            "done with p1035.jpg\n",
            "done with p2024.jpg\n",
            "done with p414.jpg\n",
            "done with p1992.jpg\n",
            "done with p1165.jpg\n",
            "done with p429.jpg\n",
            "here\n",
            "done with p1059.jpg\n",
            "done with p1984.jpg\n",
            "done with p1414.jpg\n",
            "done with p1226.jpg\n",
            "done with p150.jpg\n",
            "done with p1333.jpg\n",
            "done with p1073.jpg\n",
            "done with p1294.jpg\n",
            "done with p440.jpg\n",
            "done with p1318.jpg\n",
            "done with p1971.jpg\n",
            "done with p2022.jpg\n",
            "done with p127.jpg\n",
            "done with p467.jpg\n",
            "done with p442.jpg\n",
            "done with p1345.jpg\n",
            "done with p1336.jpg\n",
            "done with p2012.jpg\n",
            "done with p1122.jpg\n",
            "done with p1346.jpg\n",
            "done with p1207.jpg\n",
            "done with p1427.jpg\n",
            "done with p1951.jpg\n",
            "done with p417.jpg\n",
            "done with p1202.jpg\n",
            "done with p2011.jpg\n",
            "done with p118.jpg\n",
            "done with p1177.jpg\n",
            "done with p1089.jpg\n",
            "done with p1969.jpg\n",
            "done with p1127.jpg\n",
            "done with p1990.jpg\n",
            "done with p2019.jpg\n",
            "done with p1230.jpg\n",
            "done with p1183.jpg\n",
            "done with p2002.jpg\n",
            "done with p1960.jpg\n",
            "done with p1275.jpg\n",
            "done with p1277.jpg\n",
            "done with p449.jpg\n",
            "done with p112.jpg\n",
            "done with p435.jpg\n",
            "done with p1055.jpg\n",
            "done with p1354.jpg\n",
            "done with p423.jpg\n",
            "done with p124.jpg\n",
            "done with p1431.jpg\n",
            "done with p408.jpg\n",
            "done with p106.jpg\n",
            "done with p1157.jpg\n",
            "done with p1247.jpg\n",
            "done with p1445.jpg\n",
            "done with p430.jpg\n",
            "done with p107.jpg\n",
            "done with p1084.jpg\n",
            "done with p1213.jpg\n",
            "done with p1047.jpg\n",
            "done with p1042.jpg\n",
            "done with p421.jpg\n",
            "done with p1342.jpg\n",
            "done with p1288.jpg\n",
            "done with p455.jpg\n",
            "done with p1245.jpg\n",
            "done with p468.jpg\n",
            "done with p1926.jpg\n",
            "done with p1235.jpg\n",
            "done with p439.jpg\n",
            "here\n",
            "done with p411.jpg\n",
            "done with p1350.jpg\n",
            "done with p476.jpg\n",
            "done with p1108.jpg\n",
            "done with p116.jpg\n",
            "done with p456.jpg\n",
            "done with p1272.jpg\n",
            "done with p1227.jpg\n",
            "done with p1105.jpg\n",
            "done with p1449.jpg\n",
            "done with p451.jpg\n",
            "done with p1408.jpg\n",
            "done with p129.jpg\n",
            "done with p1096.jpg\n",
            "done with p1302.jpg\n",
            "done with p1332.jpg\n",
            "done with p1061.jpg\n",
            "done with p1334.jpg\n",
            "done with p1080.jpg\n",
            "done with p1180.jpg\n",
            "done with p478.jpg\n",
            "done with p474.jpg\n",
            "done with p125.jpg\n",
            "done with p1034.jpg\n",
            "done with p2003.jpg\n",
            "done with p425.jpg\n",
            "done with p1229.jpg\n",
            "done with p2000.jpg\n",
            "done with p135.jpg\n",
            "done with p1098.jpg\n",
            "done with p1349.jpg\n",
            "done with p141.jpg\n",
            "done with p2016.jpg\n",
            "done with p110.jpg\n",
            "done with p1331.jpg\n",
            "done with p1069.jpg\n",
            "done with p1079.jpg\n",
            "done with p1131.jpg\n",
            "done with p1228.jpg\n",
            "done with p1303.jpg\n",
            "done with p126.jpg\n",
            "done with p119.jpg\n",
            "done with p1137.jpg\n",
            "done with p1955.jpg\n",
            "done with p441.jpg\n",
            "done with p1232.jpg\n",
            "here\n",
            "done with p2014.jpg\n",
            "done with p1997.jpg\n",
            "done with p1932.jpg\n",
            "here\n",
            "done with p1148.jpg\n",
            "done with p1930.jpg\n",
            "done with p1305.jpg\n",
            "done with p1326.jpg\n",
            "done with p1208.jpg\n",
            "done with p1963.jpg\n",
            "done with p1120.jpg\n",
            "done with p1184.jpg\n",
            "done with p1155.jpg\n",
            "done with p1962.jpg\n",
            "done with p1154.jpg\n",
            "here\n",
            "done with p1038.jpg\n",
            "done with p1119.jpg\n",
            "done with p1216.jpg\n",
            "done with p1287.jpg\n",
            "done with p405.jpg\n",
            "done with p1083.jpg\n",
            "done with p471.jpg\n",
            "done with p1170.jpg\n",
            "done with p1998.jpg\n",
            "done with p117.jpg\n",
            "done with p1931.jpg\n",
            "done with p1957.jpg\n",
            "done with p1201.jpg\n",
            "done with p1961.jpg\n",
            "done with p452.jpg\n",
            "done with p1116.jpg\n",
            "done with p1070.jpg\n",
            "done with p121.jpg\n",
            "done with p1147.jpg\n",
            "done with p1113.jpg\n",
            "done with p1264.jpg\n",
            "done with p477.jpg\n",
            "done with p466.jpg\n",
            "done with p1103.jpg\n",
            "done with p1298.jpg\n",
            "done with p102.jpg\n",
            "done with p475.jpg\n",
            "done with p1946.jpg\n",
            "done with p454.jpg\n",
            "done with p1188.jpg\n",
            "done with p1965.jpg\n",
            "done with p1077.jpg\n",
            "done with p1036.jpg\n",
            "done with p446.jpg\n",
            "done with p1304.jpg\n",
            "done with p1174.jpg\n",
            "done with p1433.jpg\n",
            "done with p2027.jpg\n",
            "done with p1092.jpg\n",
            "done with p1049.jpg\n",
            "done with p1052.jpg\n",
            "done with p1981.jpg\n",
            "done with p2009.jpg\n",
            "done with p1321.jpg\n",
            "done with p109.jpg\n",
            "done with p1310.jpg\n",
            "done with p1231.jpg\n",
            "done with p1048.jpg\n",
            "done with p1443.jpg\n",
            "done with p1130.jpg\n",
            "done with p1106.jpg\n",
            "done with p1078.jpg\n",
            "done with p462.jpg\n",
            "done with p2013.jpg\n",
            "done with p1982.jpg\n",
            "done with p1442.jpg\n",
            "done with p1221.jpg\n",
            "done with p1450.jpg\n",
            "done with p138.jpg\n",
            "done with p1190.jpg\n",
            "done with p1043.jpg\n",
            "done with p1940.jpg\n",
            "done with p1071.jpg\n",
            "done with p1101.jpg\n",
            "done with p1428.jpg\n",
            "done with p1133.jpg\n",
            "done with p1977.jpg\n",
            "done with p147.jpg\n",
            "done with p1952.jpg\n",
            "done with p1309.jpg\n",
            "done with p1312.jpg\n",
            "done with p1263.jpg\n",
            "done with p1091.jpg\n",
            "done with p1985.jpg\n",
            "done with p1135.jpg\n",
            "done with p453.jpg\n",
            "done with p1107.jpg\n",
            "done with p424.jpg\n",
            "done with p1149.jpg\n",
            "done with p1074.jpg\n",
            "done with p1239.jpg\n",
            "done with p1356.jpg\n",
            "done with p1284.jpg\n",
            "done with p1065.jpg\n",
            "done with p1248.jpg\n",
            "done with p1041.jpg\n",
            "done with p2021.jpg\n",
            "done with p1410.jpg\n",
            "done with p1224.jpg\n",
            "done with p1983.jpg\n",
            "done with p1972.jpg\n",
            "done with p1064.jpg\n",
            "done with p1178.jpg\n",
            "done with p1440.jpg\n",
            "done with p1979.jpg\n",
            "done with p1097.jpg\n",
            "done with p1168.jpg\n",
            "done with p1236.jpg\n",
            "done with p1246.jpg\n",
            "done with p1253.jpg\n",
            "done with p1102.jpg\n",
            "done with p1144.jpg\n",
            "done with p120.jpg\n",
            "done with p415.jpg\n",
            "done with p1192.jpg\n",
            "done with p1413.jpg\n",
            "done with p1347.jpg\n",
            "done with p1251.jpg\n",
            "done with p418.jpg\n",
            "done with p444.jpg\n",
            "done with p1032.jpg\n",
            "here\n",
            "done with p1293.jpg\n",
            "done with p1995.jpg\n",
            "done with p1271.jpg\n",
            "done with p2026.jpg\n",
            "done with p1968.jpg\n",
            "done with p1274.jpg\n",
            "done with p1126.jpg\n",
            "done with p1257.jpg\n",
            "done with p1974.jpg\n",
            "done with p465.jpg\n",
            "done with p1928.jpg\n",
            "done with p1138.jpg\n",
            "done with p1297.jpg\n",
            "done with p1111.jpg\n",
            "done with p447.jpg\n",
            "done with p1185.jpg\n",
            "done with p1087.jpg\n",
            "done with p2020.jpg\n",
            "done with p1352.jpg\n",
            "done with p2023.jpg\n",
            "done with p1286.jpg\n",
            "done with p1158.jpg\n",
            "done with p1081.jpg\n",
            "done with p1949.jpg\n",
            "done with p1114.jpg\n",
            "done with p145.jpg\n",
            "done with p1054.jpg\n",
            "done with p1308.jpg\n",
            "done with p1219.jpg\n",
            "done with p1295.jpg\n",
            "done with p1970.jpg\n",
            "done with p1944.jpg\n",
            "done with p1935.jpg\n",
            "done with p1187.jpg\n",
            "done with p463.jpg\n",
            "done with p1325.jpg\n",
            "done with p432.jpg\n",
            "done with p146.jpg\n",
            "done with p2017.jpg\n",
            "here\n",
            "done with p448.jpg\n",
            "done with p1959.jpg\n",
            "done with p1172.jpg\n",
            "done with p1062.jpg\n",
            "done with p1950.jpg\n",
            "done with p2005.jpg\n",
            "done with p1939.jpg\n",
            "done with p1417.jpg\n",
            "done with p113.jpg\n",
            "done with p1953.jpg\n",
            "done with p1141.jpg\n",
            "done with p1320.jpg\n",
            "done with p1934.jpg\n",
            "done with p1316.jpg\n",
            "done with p1314.jpg\n",
            "done with p1260.jpg\n",
            "done with p111.jpg\n",
            "done with p1182.jpg\n",
            "done with p1343.jpg\n",
            "done with p1095.jpg\n",
            "done with p1072.jpg\n",
            "done with p1993.jpg\n",
            "done with p1324.jpg\n",
            "done with p1418.jpg\n",
            "done with p1058.jpg\n",
            "done with p1259.jpg\n",
            "done with p1281.jpg\n",
            "done with p1406.jpg\n",
            "done with p1051.jpg\n",
            "done with p461.jpg\n",
            "done with p2018.jpg\n",
            "done with p1956.jpg\n",
            "done with p1357.jpg\n",
            "done with p2028.jpg\n",
            "done with p1166.jpg\n",
            "done with p1337.jpg\n",
            "done with p1099.jpg\n",
            "done with p1153.jpg\n",
            "done with p142.jpg\n",
            "done with p1416.jpg\n",
            "done with p1313.jpg\n",
            "done with p1943.jpg\n",
            "done with p1267.jpg\n",
            "done with p1167.jpg\n",
            "done with p1033.jpg\n",
            "done with p1139.jpg\n",
            "done with p133.jpg\n",
            "done with p1189.jpg\n",
            "done with p1936.jpg\n",
            "done with p1173.jpg\n",
            "done with p1110.jpg\n",
            "done with p148.jpg\n",
            "done with p419.jpg\n",
            "done with p1156.jpg\n",
            "done with p427.jpg\n",
            "here\n",
            "done with p1176.jpg\n",
            "done with p1999.jpg\n",
            "done with p1243.jpg\n",
            "done with p1268.jpg\n",
            "done with p1292.jpg\n",
            "done with p1039.jpg\n",
            "here\n",
            "done with p2006.jpg\n",
            "done with p1129.jpg\n",
            "done with p2007.jpg\n",
            "done with p1265.jpg\n",
            "done with p1299.jpg\n",
            "done with p460.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "\n",
        "def count_ones(matrix):\n",
        "    count = np.count_nonzero(matrix == 1)\n",
        "    \n",
        "    return count\n",
        "\n",
        "def process_image(image):\n",
        "    current_image = test_transform(image).cuda().view(1, 3, 448, 448)\n",
        "    final_model.eval()\n",
        "    out = final_model(current_image)[\"out\"]\n",
        "    predicted_mask = torch.argmax(out, dim=1).view(448, 448).cpu()  \n",
        "    estimator = pothole_estimator(np.array(predicted_mask))\n",
        "    estimator.process_array()\n",
        "    estimator.turn_twos_to_zeros_no_diagonals()\n",
        "    final_array = estimator.turn_remaining_twos_to_ones()\n",
        "    area = count_ones(final_array)\n",
        "    \n",
        "    return area\n",
        "\n",
        "def process_images_in_directory(directory, output_csv):\n",
        "    # Open CSV file for writing\n",
        "    with open(output_csv, 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        # Write the header row\n",
        "        csvwriter.writerow(['Filename', 'Result'])\n",
        "\n",
        "        # Iterate over all files in the directory\n",
        "        for filename in os.listdir(directory):\n",
        "            \n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')):\n",
        "                print(f\"done with {filename}\")\n",
        "                filepath = os.path.join(directory, filename)\n",
        "                \n",
        "                # Open the image\n",
        "                with Image.open(filepath) as img:\n",
        "                    # Process the image\n",
        "                    img = img.convert(\"RGB\")\n",
        "                    result = process_image(img)\n",
        "                    \n",
        "                    # Write the result to the CSV file\n",
        "                    csvwriter.writerow([filename, result])\n",
        "\n",
        "\n",
        "image_directory = './data/stick subset/'\n",
        "output_csv_file = 'areas.csv'\n",
        "process_images_in_directory(image_directory, output_csv_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bags used</th>\n",
              "      <th>Pothole</th>\n",
              "      <th>real area (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50</td>\n",
              "      <td>101</td>\n",
              "      <td>3424.066312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00</td>\n",
              "      <td>102</td>\n",
              "      <td>3901.142307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.50</td>\n",
              "      <td>106</td>\n",
              "      <td>2873.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.50</td>\n",
              "      <td>107</td>\n",
              "      <td>1951.054133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>109</td>\n",
              "      <td>6072.565240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1442</td>\n",
              "      <td>3257.016884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>0.25</td>\n",
              "      <td>1443</td>\n",
              "      <td>1537.655534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1445</td>\n",
              "      <td>1344.594440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1449</td>\n",
              "      <td>1322.646696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>2.00</td>\n",
              "      <td>1450</td>\n",
              "      <td>4863.380806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>343 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Bags used   Pothole  real area (cm)\n",
              "0          0.50      101     3424.066312\n",
              "1          1.00      102     3901.142307\n",
              "2          0.50      106     2873.001900\n",
              "3          0.50      107     1951.054133\n",
              "4          0.50      109     6072.565240\n",
              "..          ...      ...             ...\n",
              "338        1.00     1442     3257.016884\n",
              "339        0.25     1443     1537.655534\n",
              "340        0.50     1445     1344.594440\n",
              "341        0.50     1449     1322.646696\n",
              "342        2.00     1450     4863.380806\n",
              "\n",
              "[343 rows x 3 columns]"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "areas_df = pd.read_csv(header=0, filepath_or_buffer=\"./areas.csv\")\n",
        "areas_df[\"Pothole\"] = areas_df[\"Filename\"].apply(lambda x: int(x[1:-4]))\n",
        "areas_df = areas_df.drop(columns=\"Filename\")\n",
        "\n",
        "result_df[\"Pothole\"] = result_df[\"image\"].apply(lambda x: int(x[6:-4]))\n",
        "distances_df = result_df.drop(columns = \"image\")\n",
        "\n",
        "real_area_df = pd.merge(areas_df, distances_df, on='Pothole', how='inner')\n",
        "\n",
        "real_area_df[\"area per pixel\"] = real_area_df[\"distance\"].apply(lambda x: 2500 / (x**2))\n",
        "real_area_df\n",
        "real_area_df['real area (cm)'] = real_area_df[\"Result\"] * real_area_df[\"area per pixel\"]\n",
        "real_area_df = real_area_df.drop(columns = [\"Result\", \"distance\", \"area per pixel\"])\n",
        "\n",
        "label_df = pd.read_csv(\"./data/train_labels.csv\", header=0)\n",
        "label_df[\"Pothole\"] = label_df[\"Pothole number\"]\n",
        "label_df = label_df.drop(columns = \"Pothole number\")\n",
        "\n",
        "df_train = pd.merge(label_df, real_area_df, on = \"Pothole\", how = \"inner\")\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f701d214190>"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "remove = np.where(df_train[\"real area (cm)\"] > 12000)[0]\n",
        "\n",
        "df_train.drop(remove, inplace=True)\n",
        "df_train = df_train.dropna()\n",
        "plt.scatter(x = df_train[\"real area (cm)\"], y = df_train[\"Bags used \"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "X = np.array(df_train[\"real area (cm)\"]).reshape(-1, 1)\n",
        "y = df_train[\"Bags used \"]\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
