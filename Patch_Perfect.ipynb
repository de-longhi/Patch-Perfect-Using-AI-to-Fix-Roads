{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion() # (%matplotlib inline)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will document the process we underwent to find a solution to the plothole-problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking at the data systematically to see where we will inevitably need to solve problems before we create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = os.path.abspath('') # notebooks are stupid\n",
    "DATA_DIR = Path(__file__).resolve() / \"data\"\n",
    "TRAIN_LABELS_PATH = DATA_DIR / \"train_labels.csv\"\n",
    "\n",
    "train_label_df = pd.read_csv(filepath_or_buffer=TRAIN_LABELS_PATH)\n",
    "train_label_df.rename(columns={'Bags used ': 'Bags used'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function install_repl_displayhook.<locals>.post_execute at 0x73253c27d200> (for post_execute):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find font DejaVu Sans:style=normal:variant=normal:weight=normal:stretch=normal:size=13.2, and fallback to the default font was disabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpost_execute\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/_pylab_helpers.py\u001b[0m in \u001b[0;36mdraw_all\u001b[0;34m(cls, force)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_title_position\u001b[0;34m(self, renderer)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_get_agg_font\u001b[0;34m(self, prop)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36mfindfont\u001b[0;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36m_findfont_cached\u001b[0;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing, rc_params)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36mfindfont\u001b[0;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hackathon/lib/python3.7/site-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36m_findfont_cached\u001b[0;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing, rc_params)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find font DejaVu Sans:style=normal:variant=normal:weight=normal:stretch=normal:size=13.2, and fallback to the default font was disabled"
     ]
    }
   ],
   "source": [
    "values = train_label_df.loc[:, 'Bags used'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(values, bins=range(1, max(values) + 2), edgecolor='black')\n",
    "plt.title('Histogram of Data Points per Bag Amount')\n",
    "plt.xlabel('Number of Data Points for Each Bag Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50     278\n",
       "0.25     140\n",
       "1.00     116\n",
       "1.50      35\n",
       "2.00      34\n",
       "3.00      12\n",
       "2.50       7\n",
       "0.55       4\n",
       "8.00       3\n",
       "1.05       3\n",
       "1.55       2\n",
       "4.00       2\n",
       "5.00       1\n",
       "15.00      1\n",
       "3.10       1\n",
       "3.05       1\n",
       "5.50       1\n",
       "7.00       1\n",
       "2.05       1\n",
       "12.00      1\n",
       "Name: Bags used, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a massive class imbalance. This could create issues where a model trained on this dataset has a bias towards more common bags. Most values are between 0 and 1 with some values much higher. There are many strategies we could use to solve this, including but not limited to: \n",
    "<ul>\n",
    "<li>Some label abstraction technique where we might create labels based on a logarithmic scale</li>\n",
    "<li>Data augmentation as a class imbalance mitigation: This process is called upsampling</li>\n",
    "</ul>\n",
    "\n",
    "We should also consider the following: The data makes this problem seem like a regression model is needed, but tuning the labels may enable us to change it to a much simpler classification task at the cost of some accuracy. Doing this would result in a much more robust model and enable us to use techniques like label smoothing to let the model generalize more to unseen data. \n",
    "<hr>\n",
    "References:\n",
    "<ul>\n",
    "<li>Paperspace Blog. (2022). Data Augmentation: A Class Imbalance Mitigative Measure. [online] Available at: https://blog.paperspace.com/data-augmentation-a-class-imbalance-mitigative-measure/.</li>\n",
    "<li>S. Wang and X. Yao, \"Multiclass Imbalance Problems: Analysis and Potential Solutions,\" in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 42, no. 4, pp. 1119-1130, Aug. 2012, doi: 10.1109/TSMCB.2012.2187280. keywords: {Training;Correlation;Training data;Pattern analysis;Genetic algorithms;IEEE Potentials;Cybernetics;Boosting;diversity;ensemble learning;multiclass imbalance problems;negative correlation learning}, </li>\n",
    "</ul>\n",
    "‌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "model = torch.load(\"./pretrained_model.pt\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    #if(\"bn\" not in name):\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "model.classifier = nn.Sequential( # Change only the classifier of the model, I.E the last few layers\n",
    "    \n",
    "    nn.Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.1, inplace=False),\n",
    "    nn.Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1)) # Change the output to 3 classes instead of 21\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers: 5\n",
      "Non-trainable layers: 157\n"
     ]
    }
   ],
   "source": [
    "trainable_layers = 0\n",
    "non_trainable_layers = 0\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    # Check if any parameter in the layer requires gradients\n",
    "    if any(param.requires_grad for param in module.parameters()):\n",
    "        trainable_layers += 1\n",
    "    else:\n",
    "        non_trainable_layers += 1\n",
    "\n",
    "print(f\"Trainable layers: {trainable_layers}\")\n",
    "print(f\"Non-trainable layers: {non_trainable_layers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 9439747\n",
      "Non-trainable parameters: 25873237\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainable_params = 0\n",
    "non_trainable_params = 0\n",
    "\n",
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()  # Count the number of elements\n",
    "    else:\n",
    "        non_trainable_params += param.numel()\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     filter(lambda p: p.requires_grad, model.parameters()), \n",
    "#     lr=0.001\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=1):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 3e-4) # Karpathy's number\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            # Move data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.type(torch.LongTensor).cuda()\n",
    "                model.cuda()\n",
    "                \n",
    "            # Forward pass\n",
    "            outputs = model(images)['out']\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_pixels = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "            outputs = model(images)['out']\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            total_pixels += labels.numel()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "        \n",
    "        accuracy = total_correct / total_pixels\n",
    "        print(f'Validation Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for your data\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "rgb_to_class = {\n",
    "    (0, 0, 0): 0,\n",
    "    (255, 255, 255): 1,\n",
    "    (100, 100, 100): 2\n",
    "}\n",
    "\n",
    "class Potholes(Dataset):\n",
    "    def __init__(self, image_dir, label_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_filenames = os.listdir(image_dir)\n",
    "        self.mean = (0.485, 0.456, 0.406)\n",
    "        self.std = (0.229, 0.224, 0.225)   \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rgb_to_mask(mask):\n",
    "        # Convert the mask to a numpy array\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        # Initialize an array to hold the class indices\n",
    "        class_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)\n",
    "        \n",
    "        # Apply the mapping from RGB values to class indices\n",
    "        for rgb, class_index in rgb_to_class.items():\n",
    "            matches = np.all(mask == rgb, axis=-1)\n",
    "            class_mask[matches] = class_index\n",
    "            \n",
    "        return class_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, (img_name[:-4] + \"_mask.png\"))\n",
    "        \n",
    "        # Load image and label\n",
    "        image = Image.open(img_path)\n",
    "        mask = Image.open(label_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transformations\n",
    "        \n",
    "        image, mask = self.image_transforms(image, mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define transforms for data augmentation and normalization\n",
    "    \n",
    "    def image_transforms(self, image, label):\n",
    "        transform_images = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),  # Resize to the desired input size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ])\n",
    "        transform_labels = transforms.Compose([\n",
    "            transforms.Resize((256, 256)) \n",
    "        ])\n",
    "        mask = self.rgb_to_mask(transform_labels(label))\n",
    "        return transform_images(image), mask\n",
    "\n",
    "# Paths to your dataset\n",
    "train_image_dir = \"./temp/subset/\"\n",
    "train_label_dir = \"./temp/masks/\" \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1.2197\n"
     ]
    }
   ],
   "source": [
    "# Create datasets and data loaders\n",
    "train_dataset = Potholes(train_image_dir, train_label_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
    "\n",
    "train_model(model, train_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/johan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'pillow>=10.3.0'] not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: gitpython>=3.1.30 in /home/johan/miniconda3/envs/hackathon/lib/python3.8/site-packages (3.1.43)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /home/johan/miniconda3/envs/hackathon/lib/python3.8/site-packages (10.4.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/johan/miniconda3/envs/hackathon/lib/python3.8/site-packages (from gitpython>=3.1.30) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/johan/miniconda3/envs/hackathon/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30) (5.0.1)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 4.5s, installed 2 packages: ['gitpython>=3.1.30', 'pillow>=10.3.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "YOLOv5 🚀 2024-8-14 Python-3.7.13 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 2060, 5918MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "stick_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
