{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i-ebO8XbEsEd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion() # (%matplotlib inline)\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy0o8aeIEsEf"
      },
      "source": [
        "# Patch Perfect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC__VfxoEsEi"
      },
      "source": [
        "This notebook will document the process we underwent to find a solution to the plothole-problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZOHtzy0EsEj"
      },
      "source": [
        "\n",
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY7iO0TbEsEj"
      },
      "source": [
        "We start by looking at the data systematically to see where we will inevitably need to solve problems before we create a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hVhsQ812EsEk"
      },
      "outputs": [],
      "source": [
        "# __file__ = os.path.abspath('') # notebooks are stupid\n",
        "# DATA_DIR = Path(__file__).resolve() / \"data\"\n",
        "# TRAIN_LABELS_PATH = DATA_DIR / \"train_labels.csv\"\n",
        "\n",
        "# train_label_df = pd.read_csv(filepath_or_buffer=TRAIN_LABELS_PATH)\n",
        "# train_label_df.rename(columns={'Bags used ': 'Bags used'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HfVHP5EDEsEl",
        "outputId": "81635175-1a4b-4146-cdea-b6fac2347de5"
      },
      "outputs": [],
      "source": [
        "# values = train_label_df.loc[:, 'Bags used'].value_counts()\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.hist(values, bins=range(1, max(values) + 2), edgecolor='black')\n",
        "# plt.title('Histogram of Data Points per Bag Amount')\n",
        "# plt.xlabel('Number of Data Points for Each Bag Amount')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMcpWCujEsEn"
      },
      "source": [
        "We can see that there is a massive class imbalance. This could create issues where a model trained on this dataset has a bias towards more common bags. Most values are between 0 and 1 with some values much higher. There are many strategies we could use to solve this, including but not limited to:\n",
        "<ul>\n",
        "<li>Some label abstraction technique where we might create labels based on a logarithmic scale</li>\n",
        "<li>Data augmentation as a class imbalance mitigation: This process is called upsampling</li>\n",
        "</ul>\n",
        "\n",
        "We should also consider the following: The data makes this problem seem like a regression model is needed, but tuning the labels may enable us to change it to a much simpler classification task at the cost of some accuracy. Doing this would result in a much more robust model and enable us to use techniques like label smoothing to let the model generalize more to unseen data.\n",
        "<hr>\n",
        "References:\n",
        "<ul>\n",
        "<li>Paperspace Blog. (2022). Data Augmentation: A Class Imbalance Mitigative Measure. [online] Available at: https://blog.paperspace.com/data-augmentation-a-class-imbalance-mitigative-measure/.</li>\n",
        "<li>S. Wang and X. Yao, \"Multiclass Imbalance Problems: Analysis and Potential Solutions,\" in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 42, no. 4, pp. 1119-1130, Aug. 2012, doi: 10.1109/TSMCB.2012.2187280. keywords: {Training;Correlation;Training data;Pattern analysis;Genetic algorithms;IEEE Potentials;Cybernetics;Boosting;diversity;ensemble learning;multiclass imbalance problems;negative correlation learning}, </li>\n",
        "</ul>\n",
        "â€Œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsDHOvH0EsEp"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kqPHOll3EsEq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_220969/3933020813.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(\"./pretrained_model.pt\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "model = torch.load(\"./pretrained_model.pt\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "      param.requires_grad = False\n",
        "\n",
        "\n",
        "model.classifier = nn.Sequential( # Change only the classifier of the model, I.E the last few layers\n",
        "\n",
        "    nn.Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
        "    nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1, inplace=False),\n",
        "    nn.Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1)) # Change the output to 3 classes instead of 21\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s-gk4B3EsEr",
        "outputId": "5ea90a64-eca6-41fd-bc95-9272a1567577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable layers: 75\n",
            "Non-trainable layers: 87\n"
          ]
        }
      ],
      "source": [
        "trainable_layers = 0\n",
        "non_trainable_layers = 0\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    # Check if any parameter in the layer requires gradients\n",
        "    if any(param.requires_grad for param in module.parameters()):\n",
        "        trainable_layers += 1\n",
        "    else:\n",
        "        non_trainable_layers += 1\n",
        "\n",
        "print(f\"Trainable layers: {trainable_layers}\")\n",
        "print(f\"Non-trainable layers: {non_trainable_layers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoRmM5y3EsEr",
        "outputId": "322824e9-04f0-4f38-cd24-5baf884d4369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 9485187\n",
            "Non-trainable parameters: 25827797\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trainable_params = 0\n",
        "non_trainable_params = 0\n",
        "\n",
        "for param in model.parameters():\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()  # Count the number of elements\n",
        "    else:\n",
        "        non_trainable_params += param.numel()\n",
        "\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Non-trainable parameters: {non_trainable_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSg-1m7EsEt"
      },
      "source": [
        "## Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ns4tJ9HoEsEt"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "# optimizer = optim.Adam(\n",
        "#     filter(lambda p: p.requires_grad, model.parameters()),\n",
        "#     lr=0.001\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=1):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = 3e-4) # Karpathy's number\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            # Move data to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.type(torch.LongTensor).cuda()\n",
        "                model.cuda()\n",
        "\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)['out']\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #running_loss += loss.item() * images.size(0)\n",
        "        test_loss = evaluate_model(model, val_loader)\n",
        "        #epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Accuracy: {test_loss:.4f}')\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            total_pixels += labels.numel()\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "\n",
        "        accuracy = total_correct / total_pixels\n",
        "        return accuracy\n",
        "        #print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmklZldlEsEu"
      },
      "source": [
        "## Data Prep Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ClR6aVHHEsEu"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class for your data\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "rgb_to_class = {\n",
        "    (0, 0, 0): 0,\n",
        "    (255, 255, 255): 1,\n",
        "    (100, 100, 100): 2\n",
        "}\n",
        "\n",
        "class JointTransform:\n",
        "    def __init__(self, image_transforms=None, mask_transforms=None):\n",
        "        self.image_transforms = image_transforms\n",
        "        self.mask_transforms = mask_transforms\n",
        "    @staticmethod\n",
        "    def set_seed(seed):\n",
        "      torch.manual_seed(seed)\n",
        "      torch.cuda.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "      random.seed(seed)\n",
        "      np.random.seed(seed)\n",
        "\n",
        "      # Ensure that all operations are deterministic\n",
        "      torch.backends.cudnn.deterministic = True\n",
        "      torch.backends.cudnn.benchmark = False\n",
        "    def __call__(self, image, mask):\n",
        "        if self.image_transforms:\n",
        "            seed = random.randint(0, 2**32)\n",
        "            self.set_seed(seed)\n",
        "            image = self.image_transforms(image)\n",
        "\n",
        "            self.set_seed(seed)\n",
        "            mask = self.mask_transforms(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "image_augmentations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "mask_augmentations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "# Instantiate joint transform\n",
        "augmentation = JointTransform(image_transforms=image_augmentations, mask_transforms=mask_augmentations)\n",
        "\n",
        "\n",
        "class Potholes(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, do_augmentation = True):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_filenames = os.listdir(image_dir)\n",
        "        self.mean = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "        self.do_augmentation = do_augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    @staticmethod\n",
        "    def rgb_to_mask(mask):\n",
        "        # Convert the mask to a numpy array\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # Initialize an array to hold the class indices\n",
        "        class_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)\n",
        "\n",
        "        # Apply the mapping from RGB values to class indices\n",
        "        for rgb, class_index in rgb_to_class.items():\n",
        "            matches = np.all(mask == rgb, axis=-1)\n",
        "            class_mask[matches] = class_index\n",
        "\n",
        "        return class_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, (img_name[:-4] + \"_mask.png\"))\n",
        "\n",
        "        # Load image and label\n",
        "        image = Image.open(img_path)\n",
        "        mask = Image.open(label_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply augmentations then transformations\n",
        "\n",
        "        if self.do_augmentation: image, mask = augmentation(image, mask)\n",
        "\n",
        "        image, mask = self.image_transforms(image, mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Define transforms for data augmentation and normalization\n",
        "\n",
        "    def image_transforms(self, image, label):\n",
        "        transform_images = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Resize to the desired input size\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "        transform_labels = transforms.Compose([\n",
        "            transforms.Resize((256, 256))\n",
        "        ])\n",
        "        mask = self.rgb_to_mask(transform_labels(label))\n",
        "        return transform_images(image), mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Paths to your dataset\n",
        "train_image_dir = \"./data/train_images_segmented/\"\n",
        "train_label_dir = \"./data/train_masks_segmented/\"\n",
        "\n",
        "val_image_dir = \"./data/validation set/\"\n",
        "val_label_dir = \"./data/validation masks/\"\n",
        "\n",
        "# size of stick/image indicates camera zoom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "gkGh36k8EsEu",
        "outputId": "a51b5930-8b0e-446c-fb47-b51823ad840f"
      },
      "outputs": [],
      "source": [
        "# Create datasets and data loaders\n",
        "train_dataset = Potholes(train_image_dir, train_label_dir, do_augmentation=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "val_dataset = Potholes(val_image_dir, val_label_dir, do_augmentation=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "#train_model(model = model,train_loader = train_loader,num_epochs= 1000, val_loader=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ExDTpR72fYNV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0):\n",
        "    model.to(\"cuda\")\n",
        "    number_in_epoch = len(train_loader) - 1\n",
        "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
        "    lr = init_value\n",
        "    optimizer.param_groups[0][\"lr\"] = lr\n",
        "    best_loss = float('inf')\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        batch_num += 1\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs, labels\n",
        "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)[\"out\"]\n",
        "        loss = loss_fn(outputs, labels.type(torch.LongTensor).cuda())\n",
        "\n",
        "        # Convert loss to float for comparison and storage\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        # Crash out if loss explodes\n",
        "        if batch_num > 1 and loss_value > 4 * best_loss:\n",
        "            return log_lrs[10:-5], losses[10:-5]\n",
        "\n",
        "        # Record the best loss\n",
        "        if loss_value < best_loss:\n",
        "            best_loss = loss_value\n",
        "\n",
        "        # Store the values\n",
        "        losses.append(loss_value)\n",
        "        log_lrs.append(math.log10(lr))\n",
        "\n",
        "        # Do the backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the lr for the next step and store\n",
        "        lr *= update_step\n",
        "        optimizer.param_groups[0][\"lr\"] = lr\n",
        "\n",
        "    return log_lrs[10:-5], losses[10:-5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#logs,losses = find_lr(model, nn.CrossEntropyLoss(), optim.Adam(model.parameters(), lr = 3e-4), train_loader=train_loader)\n",
        "#plt.plot(logs,losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_220969/1325712498.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  final_model = torch.load(\"./trained_model.pt\")\n"
          ]
        }
      ],
      "source": [
        "final_model = torch.load(\"./trained_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "test_transform = transforms.Compose([\n",
        "            transforms.Resize((448, 448)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "train_transform = transforms.Compose([\n",
        "            transforms.Resize((448, 448)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "test_image = Image.open(\"./data/test set resized/p103.jpg\").convert(\"RGB\")\n",
        "test_image = test_transform(test_image).cuda().view(1, 3, 448, 448)\n",
        "final_model.eval()\n",
        "out = final_model(test_image)[\"out\"]\n",
        "predicted_mask = torch.argmax(out, dim=1).view(448, 448).cpu()\n",
        "\n",
        "\n",
        "def array_to_image(array):\n",
        "    # Define the mapping from array values to RGB colors\n",
        "    color_mapping = {\n",
        "        0: (0, 0, 0),       # Black\n",
        "        1: (255, 255, 255), # White\n",
        "        2: (128, 128, 128)  # Gray\n",
        "    }\n",
        "    \n",
        "    # Convert the array to an RGB image\n",
        "    height, width = array.shape\n",
        "    image = Image.new('RGB', (width, height))\n",
        "    \n",
        "    for y in range(height):\n",
        "        for x in range(width):\n",
        "            image.putpixel((x, y), color_mapping[array[y, x]])\n",
        "    \n",
        "    return image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class pothole_estimator():\n",
        "    def __init__(self, array):\n",
        "        self.array = array\n",
        "        \n",
        "    def is_valid_two(self, y, x):\n",
        "        \"\"\"Check if the element at (y, x) is a 2 with both 0 and 1 as neighbors.\"\"\"\n",
        "        height, width = self.array.shape\n",
        "        neighbors = []\n",
        "        \n",
        "        # Check all 8 possible neighbors\n",
        "        for dy in [-1, 0, 1]:\n",
        "            for dx in [-1, 0, 1]:\n",
        "                if dy == 0 and dx == 0:\n",
        "                    continue\n",
        "                ny, nx = y + dy, x + dx\n",
        "                if 0 <= ny < height and 0 <= nx < width:\n",
        "                    neighbors.append(self.array[ny, nx])\n",
        "        \n",
        "        return 0 in neighbors and 1 in neighbors\n",
        "    \n",
        "\n",
        "    def find_closest_pair(self, coords):\n",
        "        \"\"\"Find the pair of points in coords that are closest to each other.\"\"\"\n",
        "        min_dist = float('inf')\n",
        "        closest_pair = None\n",
        "        \n",
        "        for i in range(len(coords)):\n",
        "            for j in range(i + 1, len(coords)):\n",
        "                if self.is_adjacent(coords[i], coords[j]):\n",
        "                    continue\n",
        "                dist = np.linalg.norm(np.array(coords[i]) - np.array(coords[j]))\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    closest_pair = (coords[i], coords[j])\n",
        "    \n",
        "        return closest_pair\n",
        "        \"\"\"Draw a line of 0's between points p1 and p2 using Bresenham's line algorithm.\"\"\"\n",
        "    def draw_line(self, p1, p2):\n",
        "        y1, x1 = p1\n",
        "        y2, x2 = p2\n",
        "        dy = abs(y2 - y1)\n",
        "        dx = abs(x2 - x1)\n",
        "        sy = 1 if y1 < y2 else -1\n",
        "        sx = 1 if x1 < x2 else -1\n",
        "        err = dx - dy\n",
        "\n",
        "        while True:\n",
        "            self.array[y1, x1] = 1\n",
        "            if (y1, x1) == (y2, x2):\n",
        "                break\n",
        "            e2 = 2 * err\n",
        "            if e2 > -dy:\n",
        "                err -= dy\n",
        "                x1 += sx\n",
        "            if e2 < dx:\n",
        "                err += dx\n",
        "                y1 += sy\n",
        "    @staticmethod        \n",
        "    def is_adjacent(p1, p2):\n",
        "        \"\"\"Check if two points p1 and p2 are adjacent (including diagonally).\"\"\"\n",
        "        y1, x1 = p1\n",
        "        y2, x2 = p2\n",
        "        return abs(y1 - y2) <= 1 and abs(x1 - x2) <= 1\n",
        "\n",
        "    def process_array(self):\n",
        "        height, width = self.array.shape\n",
        "        valid_twos = []\n",
        "        \n",
        "        # Step 1: Find all valid 2's\n",
        "        for y in range(height):\n",
        "            for x in range(width):\n",
        "                \n",
        "                if self.array[y, x] == 2 and self.is_valid_two(y, x):\n",
        "                    valid_twos.append((y, x))\n",
        "        \n",
        "        # Step 2: Draw lines between closest pairs of valid 2's\n",
        "        while len(valid_twos) > 1:\n",
        "            temp = self.find_closest_pair(valid_twos)\n",
        "            if isinstance(temp, tuple):\n",
        "                p1, p2 = self.find_closest_pair(valid_twos)\n",
        "            else:\n",
        "                print(\"here\")\n",
        "                break\n",
        "            self.draw_line(p1, p2)\n",
        "            valid_twos.remove(p1)\n",
        "            valid_twos.remove(p2)\n",
        "\n",
        "        return self.array\n",
        "    \n",
        "    def turn_twos_to_zeros_no_diagonals(self):\n",
        "        \"\"\"\n",
        "        Turn all 2's that touch a 0 into a 0, considering only horizontal and vertical neighbors,\n",
        "        and repeat the process until no pixels are changed.\n",
        "        \"\"\"\n",
        "        height, width = self.array.shape\n",
        "        changed = True\n",
        "        \n",
        "        while changed:\n",
        "            changed = False\n",
        "            new_array = self.array.copy()\n",
        "            \n",
        "            for y in range(height):\n",
        "                for x in range(width):\n",
        "                    if self.array[y, x] == 2:\n",
        "                        # Check only 4 possible neighbors (up, down, left, right)\n",
        "                        for dy, dx in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "                            ny, nx = y + dy, x + dx\n",
        "                            if 0 <= ny < height and 0 <= nx < width:\n",
        "                                if self.array[ny, nx] == 0:\n",
        "                                    new_array[y, x] = 0\n",
        "                                    changed = True\n",
        "                                    break\n",
        "                                    \n",
        "            self.array = new_array\n",
        "\n",
        "        return self.array\n",
        "    def turn_remaining_twos_to_ones(self):\n",
        "        \"\"\"\n",
        "        Turn all remaining 2's into 1's.\n",
        "        \"\"\"\n",
        "        self.array[self.array == 2] = 1\n",
        "        return self.array\n",
        "\n",
        "\n",
        "estimator = pothole_estimator(np.array(predicted_mask))\n",
        "estimator.process_array()\n",
        "estimator.turn_twos_to_zeros_no_diagonals()\n",
        "final_array = estimator.turn_remaining_twos_to_ones()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "array_to_image(np.array(predicted_mask)).save(\"output.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "def calculate_distance(x1, y1, x2, y2):\n",
        "    return sqrt((x2 - x1) ** 2 + (y1 - y2) ** 2)\n",
        "\n",
        "def process_data(data):\n",
        "    records = []\n",
        "    \n",
        "    for image_name, group in data.groupby('image'):\n",
        "        points = group[['x', 'y']].values\n",
        "        \n",
        "        if len(points) == 2:\n",
        "            distance = calculate_distance(points[0][0], points[0][1], points[1][0], points[1][1])\n",
        "        elif len(points) == 3:\n",
        "            d1 = calculate_distance(points[0][0], points[0][1], points[1][0], points[1][1])\n",
        "            d2 = calculate_distance(points[0][0], points[0][1], points[2][0], points[2][1])\n",
        "            d3 = calculate_distance(points[1][0], points[1][1], points[2][0], points[2][1])\n",
        "            distance = np.mean(sorted([d1, d2, d3])[:2])\n",
        "        else:\n",
        "            distance = None  # Handle cases with more or less than 2 or 3 points\n",
        "        \n",
        "        records.append({'image': image_name, 'distance': distance})\n",
        "    \n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Replace 'data.csv' with your actual CSV file path\n",
        "csv_file_path = './test_cluster_centroids.csv'\n",
        "\n",
        "# Read the data from the CSV file\n",
        "df_test_cent = pd.read_csv(csv_file_path, header=0, names=['image', 'point', 'x', 'y'])\n",
        "\n",
        "# Ensure the x and y columns are floats\n",
        "df_test_cent['x'] = df_test_cent['x'].astype(float)\n",
        "df_test_cent['y'] = df_test_cent['y'].astype(float)\n",
        "df_test_cent['point'] = df_test_cent['point'].astype(int)\n",
        "\n",
        "# Replace 'data.csv' with your actual CSV file path\n",
        "csv_file_path = './cluster_centroids.csv'\n",
        "\n",
        "# Read the data from the CSV file\n",
        "df_train_cent = pd.read_csv(csv_file_path, header=0, names=['image', 'point', 'x', 'y'])\n",
        "\n",
        "# Ensure the x and y columns are floats\n",
        "df_train_cent ['x'] = df_train_cent ['x'].astype(float)\n",
        "df_train_cent ['y'] = df_train_cent ['y'].astype(float)\n",
        "df_train_cent ['point'] = df_train_cent ['point'].astype(int)\n",
        "\n",
        "\n",
        "train_dists = process_data(df_train_cent)\n",
        "test_dists = process_data(df_test_cent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done with p1291.jpg\n",
            "done with p2008.jpg\n",
            "done with p1256.jpg\n",
            "done with p1994.jpg\n",
            "done with p1341.jpg\n",
            "done with p1986.jpg\n",
            "done with p1234.jpg\n",
            "done with p1306.jpg\n",
            "here\n",
            "done with p1142.jpg\n",
            "done with p2004.jpg\n",
            "done with p1118.jpg\n",
            "done with p1412.jpg\n",
            "done with p1942.jpg\n",
            "done with p136.jpg\n",
            "done with p1425.jpg\n",
            "done with p1424.jpg\n",
            "done with p1242.jpg\n",
            "done with p1100.jpg\n",
            "done with p1193.jpg\n",
            "done with p431.jpg\n",
            "done with p1195.jpg\n",
            "done with p413.jpg\n",
            "done with p132.jpg\n",
            "done with p1112.jpg\n",
            "done with p1948.jpg\n",
            "done with p438.jpg\n",
            "done with p2029.jpg\n",
            "done with p1088.jpg\n",
            "done with p123.jpg\n",
            "done with p1171.jpg\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m image_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/train_images_segmented/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m output_csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./train_areas.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mprocess_images_in_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m image_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/test set resized/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     52\u001b[0m output_csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test_areas.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "Cell \u001b[0;32mIn[18], line 41\u001b[0m, in \u001b[0;36mprocess_images_in_directory\u001b[0;34m(directory, output_csv)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(filepath) \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Process the image\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Write the result to the CSV file\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     csvwriter\u001b[38;5;241m.\u001b[39mwriterow([filename, result])\n",
            "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     15\u001b[0m estimator \u001b[38;5;241m=\u001b[39m pothole_estimator(np\u001b[38;5;241m.\u001b[39marray(predicted_mask))\n\u001b[1;32m     16\u001b[0m estimator\u001b[38;5;241m.\u001b[39mprocess_array()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mturn_twos_to_zeros_no_diagonals\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m final_array \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mturn_remaining_twos_to_ones()\n\u001b[1;32m     19\u001b[0m area \u001b[38;5;241m=\u001b[39m count_ones(final_array)\n",
            "Cell \u001b[0;32mIn[15], line 105\u001b[0m, in \u001b[0;36mpothole_estimator.turn_twos_to_zeros_no_diagonals\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(height):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(width):\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m[y, x] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;66;03m# Check only 4 possible neighbors (up, down, left, right)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m dy, dx \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)]:\n\u001b[1;32m    108\u001b[0m                 ny, nx \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m+\u001b[39m dy, x \u001b[38;5;241m+\u001b[39m dx\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "\n",
        "def count_ones(matrix):\n",
        "    count = np.count_nonzero(matrix == 1)\n",
        "    \n",
        "    return count\n",
        "\n",
        "def process_image(image):\n",
        "    current_image = test_transform(image).cuda().view(1, 3, 448, 448)\n",
        "    final_model.eval()\n",
        "    out = final_model(current_image)[\"out\"]\n",
        "    predicted_mask = torch.argmax(out, dim=1).view(448, 448).cpu()  \n",
        "    estimator = pothole_estimator(np.array(predicted_mask))\n",
        "    estimator.process_array()\n",
        "    estimator.turn_twos_to_zeros_no_diagonals()\n",
        "    final_array = estimator.turn_remaining_twos_to_ones()\n",
        "    area = count_ones(final_array)\n",
        "    \n",
        "    return area\n",
        "\n",
        "def process_images_in_directory(directory, output_csv):\n",
        "    # Open CSV file for writing\n",
        "    with open(output_csv, 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        # Write the header row\n",
        "        csvwriter.writerow(['Filename', 'Result'])\n",
        "\n",
        "        # Iterate over all files in the directory\n",
        "        for filename in os.listdir(directory):\n",
        "            \n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')):\n",
        "                print(f\"done with {filename}\")\n",
        "                filepath = os.path.join(directory, filename)\n",
        "                \n",
        "                # Open the image\n",
        "                with Image.open(filepath) as img:\n",
        "                    # Process the image\n",
        "                    img = img.convert(\"RGB\")\n",
        "                    result = process_image(img)\n",
        "                    \n",
        "                    # Write the result to the CSV file\n",
        "                    csvwriter.writerow([filename, result])\n",
        "\n",
        "\n",
        "image_directory = './data/train_images_segmented/'\n",
        "output_csv_file = './train_areas.csv'\n",
        "process_images_in_directory(image_directory, output_csv_file)\n",
        "\n",
        "image_directory = './data/test set resized/'\n",
        "output_csv_file = './test_areas.csv'\n",
        "process_images_in_directory(image_directory, output_csv_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bags used</th>\n",
              "      <th>Pothole</th>\n",
              "      <th>real area (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50</td>\n",
              "      <td>101</td>\n",
              "      <td>3457.527494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00</td>\n",
              "      <td>102</td>\n",
              "      <td>3904.474652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.50</td>\n",
              "      <td>106</td>\n",
              "      <td>2719.595535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.50</td>\n",
              "      <td>107</td>\n",
              "      <td>1926.177488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>109</td>\n",
              "      <td>5838.329081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1442</td>\n",
              "      <td>3299.007824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>0.25</td>\n",
              "      <td>1443</td>\n",
              "      <td>1553.960564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1445</td>\n",
              "      <td>1365.783991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1449</td>\n",
              "      <td>1321.783099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>2.00</td>\n",
              "      <td>1450</td>\n",
              "      <td>4911.550124</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>343 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Bags used   Pothole  real area (cm)\n",
              "0          0.50      101     3457.527494\n",
              "1          1.00      102     3904.474652\n",
              "2          0.50      106     2719.595535\n",
              "3          0.50      107     1926.177488\n",
              "4          0.50      109     5838.329081\n",
              "..          ...      ...             ...\n",
              "338        1.00     1442     3299.007824\n",
              "339        0.25     1443     1553.960564\n",
              "340        0.50     1445     1365.783991\n",
              "341        0.50     1449     1321.783099\n",
              "342        2.00     1450     4911.550124\n",
              "\n",
              "[343 rows x 3 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_areas_df = pd.read_csv(header=0, filepath_or_buffer=\"./train_areas.csv\")\n",
        "train_areas_df[\"Pothole\"] = train_areas_df[\"Filename\"].apply(lambda x: int(x[1:-4]))\n",
        "train_areas_df = train_areas_df.drop(columns=\"Filename\")\n",
        "\n",
        "train_dists[\"Pothole\"] = train_dists[\"image\"].apply(lambda x: int(x[6:-4]))\n",
        "train_dists = train_dists.drop(columns = \"image\")\n",
        "\n",
        "real_area_train_df = pd.merge(train_areas_df, train_dists, on='Pothole', how='inner')\n",
        "\n",
        "real_area_train_df[\"area per pixel\"] = real_area_train_df[\"distance\"].apply(lambda x: 2500 / (x**2))\n",
        "real_area_train_df['real area (cm)'] = real_area_train_df[\"Result\"] * real_area_train_df[\"area per pixel\"]\n",
        "real_area_train_df = real_area_train_df.drop(columns = [\"Result\", \"distance\", \"area per pixel\"])\n",
        "\n",
        "train_label_df = pd.read_csv(\"./data/train_labels.csv\", header=0)\n",
        "train_label_df[\"Pothole\"] = train_label_df[\"Pothole number\"]\n",
        "train_label_df = train_label_df.drop(columns = \"Pothole number\")\n",
        "\n",
        "df_train = pd.merge(train_label_df, real_area_train_df, on = \"Pothole\", how = \"inner\")\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "remove = np.where((df_train[\"real area (cm)\"] > 12000))[0]\n",
        "\n",
        "mean = 0\n",
        "std_dev = 0.05\n",
        "df_train.drop(remove, inplace=True)\n",
        "df_train = df_train.dropna()\n",
        "noise = np.random.normal(mean, std_dev, size=df_train.shape[0])\n",
        "df_train[\"bags with noise\"] = df_train[\"Bags used \"] + noise\n",
        "plt.scatter(x = df_train[\"real area (cm)\"], y = df_train[\"bags with noise\"])\n",
        "plt.show()\n",
        "#df_train[\"real area (cm)\"].value_counts()\n",
        "X_log = np.sqrt(df_train[\"real area (cm)\"])\n",
        "X_power = df_train[\"real area (cm)\"] ** 0.3\n",
        "y_log = np.log(df_train[\"Bags used \"])\n",
        "\n",
        "#X_binned = pd.cut(np.array(df_train[\"real area (cm)\"]).flatten(), bins=[0, 1900, 2500, np.inf], labels=[0, 1, 2])\n",
        "\n",
        "\n",
        "plt.scatter(x = X_binned, y = df_train[\"Bags used \"] + noise)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "X = np.array(df_train[\"real area (cm)\"]).reshape(-1, 1)\n",
        "y = df_train[\"Bags used \"]\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_areas_df = pd.read_csv(header=0, filepath_or_buffer=\"./test_areas.csv\")\n",
        "test_areas_df[\"Pothole\"] = test_areas_df[\"Filename\"].apply(lambda x: int(x[1:-4]))\n",
        "test_areas_df = test_areas_df.drop(columns=\"Filename\")\n",
        "\n",
        "test_dists[\"Pothole\"] = test_dists[\"image\"].apply(lambda x: int(x[6:-4]))\n",
        "test_dist = test_dists.drop(columns = \"image\")\n",
        "\n",
        "test_real_area_df = pd.merge(test_areas_df, test_dist, on='Pothole', how='inner')\n",
        "\n",
        "test_real_area_df[\"area per pixel\"] = test_real_area_df[\"distance\"].apply(lambda x: 2500 / (x**2))\n",
        "test_real_area_df\n",
        "test_real_area_df['real area (cm)'] = test_real_area_df[\"Result\"] * test_real_area_df[\"area per pixel\"]\n",
        "test_real_area_df = test_real_area_df.drop(columns = [\"Result\", \"distance\", \"area per pixel\"])\n",
        "\n",
        "X_test = test_real_area_df[\"real area (cm)\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     6949.936413\n",
              "1     2037.261587\n",
              "2      656.741431\n",
              "3     2511.048279\n",
              "4     1372.262350\n",
              "5     5480.241715\n",
              "6     1880.551926\n",
              "7     2592.721588\n",
              "8      725.387475\n",
              "9      717.015595\n",
              "10     681.183441\n",
              "11    2332.102099\n",
              "12    7552.307929\n",
              "13     821.845928\n",
              "14    1348.053057\n",
              "15     194.010311\n",
              "16    2005.610336\n",
              "17    1752.594955\n",
              "18    1643.469359\n",
              "19    3015.613153\n",
              "20    5207.182659\n",
              "21    2507.781210\n",
              "22     257.157715\n",
              "23    1462.163713\n",
              "24     927.136930\n",
              "25     879.201492\n",
              "26    1321.018670\n",
              "27     911.337537\n",
              "28    1265.926651\n",
              "29    3921.241699\n",
              "Name: real area (cm), dtype: float64"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.68758276, 0.64539602, 0.35252913, 0.74590627, 0.50432147,\n",
              "       1.37579816, 0.61215125, 0.76323265, 0.36709187, 0.36531584,\n",
              "       0.35771432, 0.7079442 , 1.81537131, 0.3875548 , 0.49918565,\n",
              "       0.25436423, 0.63868145, 0.58500615, 0.56185598, 0.85294589,\n",
              "       1.31787075, 0.74521319, 0.26776047, 0.52339336, 0.40989148,\n",
              "       0.39972234, 0.49345051, 0.40653976, 0.48176316, 1.04506812])"
            ]
          },
          "execution_count": 177,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear_model.predict(np.array(X_test).reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bags used</th>\n",
              "      <th>Pothole</th>\n",
              "      <th>real area (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50</td>\n",
              "      <td>101</td>\n",
              "      <td>3424.066312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00</td>\n",
              "      <td>102</td>\n",
              "      <td>3901.142307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.50</td>\n",
              "      <td>106</td>\n",
              "      <td>2873.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.50</td>\n",
              "      <td>107</td>\n",
              "      <td>1951.054133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>109</td>\n",
              "      <td>6072.565240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1442</td>\n",
              "      <td>3257.016884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>0.25</td>\n",
              "      <td>1443</td>\n",
              "      <td>1537.655534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1445</td>\n",
              "      <td>1344.594440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1449</td>\n",
              "      <td>1322.646696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>2.00</td>\n",
              "      <td>1450</td>\n",
              "      <td>4863.380806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>337 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Bags used   Pothole  real area (cm)\n",
              "0          0.50      101     3424.066312\n",
              "1          1.00      102     3901.142307\n",
              "2          0.50      106     2873.001900\n",
              "3          0.50      107     1951.054133\n",
              "4          0.50      109     6072.565240\n",
              "..          ...      ...             ...\n",
              "338        1.00     1442     3257.016884\n",
              "339        0.25     1443     1537.655534\n",
              "340        0.50     1445     1344.594440\n",
              "341        0.50     1449     1322.646696\n",
              "342        2.00     1450     4863.380806\n",
              "\n",
              "[337 rows x 3 columns]"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_sorted = test_real_area_df.sort_values(by=\"Pothole\")[\"real area (cm)\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = pd.DataFrame(linear_model.predict(np.array(X_test_sorted).reshape(-1, 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions.to_csv(\"submission.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
